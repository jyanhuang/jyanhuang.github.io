<!DOCTYPE  html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>SIDNet: A single image dedusting network with color cast correction</title><meta name="author" content="Jiayan Huang"/><meta name="keywords" content="&quot;Image dedusting&quot;; &quot;Deep learning&quot;; &quot;Color cast correction&quot;; &quot;Dusty image synthesis&quot;"/><meta name="description" content="Signal Processing, 199 (2022) 108612. doi:10.1016/j.sigpro.2022.108612"/><style type="text/css"> * {margin:0; padding:0; text-indent:0; }
 .a { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s2 { color: #0080AC; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s3 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 14pt; }
 .s4 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 13.5pt; }
 .s6 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; vertical-align: 4pt; }
 .s7 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s8 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s9 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10.5pt; }
 .s10 { color: #0080AC; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s11 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: 2pt; }
 .s12 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; }
 .s13 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s14 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s15 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s16 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 7pt; }
 h1 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .p, p { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; margin:0pt; }
 .s17 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s18 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s19 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: 2pt; }
 .s20 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .h2, h2 { color: black; font-family:Cambria, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 6pt; }
 .s21 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s22 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s23 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s24 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s25 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s26 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s27 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s28 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s29 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 9pt; }
 .s30 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7pt; }
 .s31 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6.5pt; }
 .s32 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s33 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s34 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s35 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; vertical-align: 1pt; }
 .s36 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4pt; }
 .s37 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -1pt; }
 .s38 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 5pt; }
 .s39 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s40 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s41 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 6pt; }
 .s42 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 3pt; }
 .s43 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 3pt; }
 .s44 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s45 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s46 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: 1pt; }
 .s47 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s48 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 8pt; }
 .s49 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s50 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: bold; text-decoration: none; font-size: 8pt; }
 .s51 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 2pt; }
 .s52 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -3pt; }
 .s53 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -1pt; }
 .s54 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s55 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 3pt; }
 .s56 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s57 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s58 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -2pt; }
 .s59 { color: black; font-family:Arial, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; vertical-align: 2pt; }
 .s60 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 9.5pt; }
 .s61 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s62 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s63 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 10pt; }
 .s64 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 2pt; }
 .s65 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt; vertical-align: 8pt; }
 .s66 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 10pt; }
 .s67 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s68 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 .s69 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 4.5pt; }
 .s70 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s71 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -4pt; }
 .s72 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s73 { color: black; font-family:"Times New Roman", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; }
 .s74 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -5pt; }
 .s75 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -4pt; }
 .s76 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: -6pt; }
 .s77 { color: black; font-family:"Arial Unicode MS", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 2pt; }
 .s78 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 8.5pt; }
 .s79 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s80 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8.5pt; vertical-align: 1pt; }
 .s81 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; vertical-align: -3pt; }
 .s82 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s83 { color: #0080AC; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s84 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s85 { color: black; font-family:Arial, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt; }
 .s86 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: -1pt; }
 .s87 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s88 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 7.5pt; }
 .s89 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 4.5pt; vertical-align: -1pt; }
 .s90 { color: black; font-family:Verdana, sans-serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 7.5pt; }
 .s91 { color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: underline; font-size: 4.5pt; vertical-align: -1pt; }
 .s92 { color: black; font-family:"Lucida Sans Unicode", sans-serif; font-style: normal; font-weight: normal; text-decoration: underline; font-size: 6pt; }
 .s93 { color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 li {display: block; }
 #l1 {padding-left: 0pt;counter-reset: c1 0; }
 #l1> li:before {counter-increment: c1; content: counter(c1, decimal)". "; color: black; font-family:Cambria, serif; font-style: normal; font-weight: bold; text-decoration: none; font-size: 8pt; }
 #l2 {padding-left: 0pt;counter-reset: d1 0; }
 #l2> li:before {counter-increment: d1; content: "("counter(d1, decimal)") "; color: black; font-family:"Palatino Linotype", serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l3 {padding-left: 0pt;counter-reset: c2 0; }
 #l3> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l4 {padding-left: 0pt;counter-reset: e1 3; }
 #l4> li:before {counter-increment: e1; content: counter(e1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l5 {padding-left: 0pt;counter-reset: e2 0; }
 #l5> li:before {counter-increment: e2; content: counter(e1, decimal)"."counter(e2, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l6 {padding-left: 0pt;counter-reset: c2 0; }
 #l6> li:before {counter-increment: c2; content: counter(c1, decimal)"."counter(c2, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l7 {padding-left: 0pt;counter-reset: c3 0; }
 #l7> li:before {counter-increment: c3; content: counter(c1, decimal)"."counter(c2, decimal)"."counter(c3, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l8 {padding-left: 0pt;counter-reset: f1 4; }
 #l8> li:before {counter-increment: f1; content: counter(f1, decimal)" "; color: black; font-style: normal; font-weight: normal; text-decoration: none; }
 #l9 {padding-left: 0pt;counter-reset: f2 4; }
 #l9> li:before {counter-increment: f2; content: counter(f1, decimal)"."counter(f2, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
 #l10 {padding-left: 0pt;counter-reset: f3 0; }
 #l10> li:before {counter-increment: f3; content: counter(f1, decimal)"."counter(f2, decimal)"."counter(f3, decimal)". "; color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 8pt; }
</style></head><body><p style="padding-top: 3pt;padding-left: 2pt;text-indent: 0pt;text-align: center;"><a href="https://doi.org/10.1016/j.sigpro.2022.108612" class="a" target="_blank">Signal Processing 199 (2022) 108612</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="608" height="0" alt="image" src="Online版本/Image_001.png"/></span></p><p style="text-indent: 0pt;line-height: 9pt;text-align: center;"><a href="http://www.ScienceDirect.com/" style=" color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank">Contents lists available at  </a><a href="http://www.ScienceDirect.com/" class="s2" target="_blank">ScienceDirect</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s3" style="text-indent: 0pt;text-align: center;">Signal Processing</p><p style="padding-top: 12pt;text-indent: 0pt;text-align: center;"><a href="http://www.elsevier.com/locate/sigpro" style=" color: black; font-family:Tahoma, sans-serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 8pt;" target="_blank">journal homepage:  </a><a href="http://www.elsevier.com/locate/sigpro" class="s2" target="_blank">www.elsevier.com/locate/sigpro</a></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 71pt;text-align: left;"><span><img width="80" height="87" alt="Elsevier logo" title="Elsevier logo" src="Online版本/Image_002.jpg"/></span><span style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 10pt;">		</span><span><img width="71" height="95" alt="Journal name" title="Journal name" src="Online版本/Image_003.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 3pt;text-align: left;"><span><img width="698" height="4" alt="image" src="Online版本/Image_004.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s4" style="padding-left: 6pt;text-indent: 0pt;line-height: 30pt;text-align: left;"><a name="bookmark0">SIDNet: A single image dedusting network with color cast correction </a><span><img width="37" height="37" alt="image" src="Online版本/Image_005.png"/></span></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark28" class="s9">Jiayan Huang </a><a href="#bookmark28" class="s6">a</a><a href="#bookmark29" class="s7">,</a><a href="#bookmark29" class="s8">b</a><a href="#bookmark30" class="s9">, Haiping Xu </a><a href="#bookmark30" class="s6">c</a><a href="#bookmark31" class="s9">, Guanghai Liu</a><a href="#bookmark31" class="s6">d</a><a href="#bookmark32" class="s9">, Chuansheng Wang </a><a href="#bookmark32" class="s6">e</a><a href="#bookmark33" class="s9">, Zhongyi Hu </a><a href="#bookmark33" class="s6">f</a><a href="#bookmark33" class="s9">,</a></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 14pt;text-align: left;"><a href="#bookmark28" class="s9">Zuoyong Li</a><a href="#bookmark28" class="s6">a</a><a href="#bookmark34" class="s7">,</a><a href="#bookmark34" class="s10">∗</a></p><p class="s11" style="padding-top: 4pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark28">a</a><span class="s12"> </span><span class="s13">Fujian Provincial Key Laboratory of Information Processing and Intelligent Control, College of Computer and Control Engineering, Minjiang University,</span></p><p class="s13" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark29">Fuzhou 350121, China</a></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark30">b</a><span class="s12"> </span><span class="s13">College of Computer and Data Science, Fuzhou University, Fuzhou 350108, China</span></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark31">c</a><span class="s12"> </span><span class="s13">College of Mathematics and Data Science, Minjiang University, Fuzhou 350121, China</span></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark32">d</a><span class="s12"> </span><span class="s13">College of Computer Science and Engineering, Guangxi Normal University, Guilin 541004, China</span></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;text-align: left;"><a name="bookmark33">e</a><span class="s12"> </span><span class="s13">Department of Systems Engineering, Automation and Industrial Informatics, Polytechnic University of Catalonia, Barcelona, Spain</span></p><p class="s11" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">f<span class="s12"> </span><span class="s13">Intelligent Information Systems Institute, Wenzhou University, Wenzhou 325035, China</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="Online版本/Image_006.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s14" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">a r t i c l e i n f o</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="178" height="1" alt="image" src="Online版本/Image_007.png"/></span></p><p class="s13" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Article history:</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Received 7 October 2021</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Revised 7 May 2022</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Accepted 8 May 2022</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">Available online 11 May 2022</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="178" height="1" alt="image" src="Online版本/Image_008.png"/></span></p><p class="s13" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Keywords:</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;line-height: 106%;text-align: left;">Image dedusting Deep learning</p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;line-height: 106%;text-align: left;">Color cast correction Dusty image synthesis</p><p class="s14" style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">a b s t r a c t</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="475" height="1" alt="image" src="Online版本/Image_009.png"/></span></p><p class="s16" style="padding-top: 1pt;padding-left: 6pt;text-indent: 0pt;text-align: justify;">Dust degrades image content and causes image color cast, which negatively impacts on many high-level computer vision tasks. In this paper, we proposed a dedusting network with color cast correction for a single dusty image (SIDNet). The SIDNet contains several dust-aware representation extraction (DustAre) modules with the same structure. Each DustAre module contains two branches. The ﬁrst branch encodes the input to estimate global veiling-light and local spatial information. The second branch generates a dust-aware map and fuses the global veiling-light, the local spatial information and the dust-aware map to generate the output. To further improve real dusty image dedusting performance, the SIDNet intro- duces a color cast correction scheme to our neural network. After considering that the average chro- maticity values of a dusty image in CIELAB color space are usually larger than those of a clean (dust-free) image, the SIDNet deﬁnes a new loss function to better guide the network training. Additionally, we also construct a new synthetic dusty image dataset for network training, which additionally considers the scene depth relationship between real dusty image and dust-free image. Experiments on synthetic and real dusty images show that the SIDNet achieves better dedusting performance compared to state-of-the- art image restoration methods.</p><p class="s16" style="padding-top: 4pt;text-indent: 0pt;text-align: right;">© 2022 Elsevier B.V. All rights reserved.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="694" height="1" alt="image" src="Online版本/Image_010.png"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l1"><li style="padding-top: 3pt;padding-left: 16pt;text-indent: -10pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark1">Introduction</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 6pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark68" class="s93">In recent years, sand-dust storm weather has frequently been increasing, which negatively impacts on the ability of monitoring systems, such as license plate detection of an outdoor monitor- ing device and objective recognition technology of automatic driv- ing. Images captured in sand-dust storm weather often depict color cast and lack contrast due to the scattering and absorption of light as it propagates through the dust. Dusty image input will make the other computer vision tasks diﬃcult, such as image retrieve tasks </a>[1]<a href="#bookmark70" class="s93">, image classiﬁcation </a>[2]<a href="#bookmark71" class="s93">, and image salient region detec- tion </a>[3]<a href="#bookmark35" class="s93">. Therefore, image dedusting as one pre-processing step of high-level computer vision tasks, has great signiﬁcance to the ﬁeld of computer vision. </a><a href="#bookmark35" class="s17">Fig. </a>1 <a href="#bookmark35" class="s93">shows several real dusty and clean (dust- free) images, as well as their corresponding RGB color histograms. From </a><a href="#bookmark35" class="s17">Fig. </a>1<span style=" color: #000;">, it can be founded that the RGB colors of dusty im-</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img width="49" height="0" alt="image" src="Online版本/Image_011.png"/></span></p><p class="s19" style="padding-top: 1pt;padding-left: 12pt;text-indent: 0pt;text-align: left;"><a name="bookmark34">∗ </a><span class="s15">Corresponding author.</span></p><p class="s20" style="padding-top: 1pt;padding-left: 18pt;text-indent: 0pt;text-align: left;"><a href="mailto:fzulzytdq@126.com" style=" color: black; font-family:Cambria, serif; font-style: italic; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">E-mail address: </a>fzulzytdq@126.com <span style=" color: #000;">(Z. Li).</span></p><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">ages are usually concentrated in a certain subrange of their corre- sponding color histogram, and the distributions of R, G, B colors are relatively separated. On the contrary, the R, G, B colors of dust- free images are more evenly distributed in the whole range (i.e., [0, 255]), and the distributions of the three colors tend to overlap. Image dedusting aims to remove dust from a dusty image while maintaining image objects in their initial natural color. Existing im- age restoration methods can be divided into traditional color cast correction methods and deep learning-based methods.</p><p class="s18" style="padding-left: 6pt;text-indent: 11pt;text-align: justify;"><a href="#bookmark72" class="s93">Traditional color cast correction methods </a>[4,5] <span style=" color: #000;">usually ﬁrst con- vert a dusty image into CIELAB color space with lightness and chromatic components separately. Then, they employ color cast correction on two chromatic components and detail enhancement on the lightness component to obtain the corresponding dust-free image. However, traditional image dedusting methods only focus on image color contrast adjustment and do not consider the re- lationship between dusty image scene depth and dust-free image scene depth. Therefore, the restoration results of traditional meth-</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="https://doi.org/10.1016/j.sigpro.2022.108612" class="a" target="_blank">https://doi.org/10.1016/j.sigpro.2022.108612</a></p><p class="s15" style="padding-left: 6pt;text-indent: 0pt;text-align: left;">0165-1684/© 2022 Elsevier B.V. All rights reserved.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 39pt;text-indent: 0pt;line-height: 93pt;text-align: left;"><span><img width="608" height="124" alt="image" src="Online版本/Image_012.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 106%;text-align: left;"><a name="bookmark35">Fig. 1. </a><span class="s15">Three pairs of real dusty images and dust-free images in similar scenes. The ﬁrst row exhibits the six original images, and the second row exhibits their corresponding RGB color histograms.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: left;">ods often suffer from low color saturation and still with lots of residual dust.</p><p class="s18" style="padding-left: 7pt;text-indent: 11pt;text-align: justify;"><a href="#bookmark73" class="s93">Recently, deep learning technology has achieved effective appli- cation on the restoration of hazy images </a>[6–8]<a href="#bookmark74" class="s93">, rainy images </a>[9,10]<a href="#bookmark77" class="s93">, underwater images </a>[11] <a href="#bookmark80" class="s93">and dusty images </a>[12]<a href="#bookmark81" class="s93">. Deep learning- based methods usually design a convolution neural network for learning image features and training the network on a large-scale image dataset for generating the restoration image </a>[13]<a href="#bookmark80" class="s93">. Since most existing networks are training on synthetic dusty image datasets, deep learning-based methods easily generate good effect on syn- thetic datasets. However, they show poor effect on real images </a>[12]<span style=" color: #000;">. Additionally, since there are few dusty image datasets for net- work training, most existing image dedusting research remains tra- ditional.</span></p><p class="s18" style="padding-left: 7pt;text-indent: 11pt;text-align: justify;"><a href="#bookmark72" class="s93" name="bookmark2">Inspired by traditional color contrast correction methods </a>[4,14,15] <a href="#bookmark82" class="s93">and existing deep learning-based image dehazing net- works </a>[16]<span style=" color: #000;">, we presented a network with color cast correction for single image dedusting (brieﬂy called SIDNet). The SIDNet uses several same dust-aware representation extraction (DustAre) mod- ules to downsample a dusty image to extract image features and then upsample them to obtain the ﬁnal dedusting result. Each DustAre module contains two branches to estimate global veiling- light, local spatial information, and a dust-aware map, and then fuse them to generate the output. Furthermore, to improve the de- dusting performance on real dusty images, the SIDNet introduces a color cast correction scheme. After comprehensively observing the differences between a dusty image and a dust-free image in their CIELAB color spaces, the SIDNet designs a new loss function to bet- ter guide the network training. In addition, due to the challenge of acquiring realistic dusty images, we take the relationship between real dusty image and dust-free image into account and construct a new dusty image synthetic dataset with both outdoor and in- door scenes for the network training. Quantitative and qualitative experimental results on a series of synthetic and real dusty images show that the SIDNet realized effect image dedusting compared to several state-of-the-art image restoration methods.</span><a name="bookmark3">&zwnj;</a><a name="bookmark36">&zwnj;</a><a name="bookmark37">&zwnj;</a></p><p style="padding-left: 19pt;text-indent: 0pt;line-height: 11pt;text-align: left;">Overall, the main contributions of this paper are as follows:</p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l2"><li style="padding-left: 20pt;text-indent: -14pt;line-height: 11pt;text-align: justify;"><p style="display: inline;">Inspired by image dehazing, we presented a network with color cast correction for image dedusting (SIDNet).</p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">The proposed SIDNet embeds several dust-aware representation extraction (DustAre) modules with the same structure, which is used to estimate and fuse the global veiling-light, the local spa- tial information, and the dust-aware map for improving image dedusting effect.</p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;"><a name="bookmark38">Considering  that  most  existing  deep  learning-based  image restoration methods have achieved good effect on synthetic im- ages and poor effect on real images, as well as for avoiding network being excessively affected by real ground truth images with color cast, the SIDNet introduces a color cast correction scheme for those ground truth images from training dataset to meet both of synthetic and real dusty image dedusting perfor- mances.</a></p></li><li style="padding-top: 2pt;padding-left: 20pt;text-indent: -14pt;line-height: 11pt;text-align: justify;"><p style="display: inline;">After observing that dusty image usually has larger <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>average chromaticity values than the dust-free image in CIELAB color space, the proposed network designs a new loss function to better guide network training.</p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">To enrich dusty image datasets for network training, we con- sidered the relationship between dusty image scene depth and dust-free image scene depth, and constructed a synthetic dusty image dataset with both outdoor and indoor scenes for deep learning-based image dedusting networks training.</p></li><li style="padding-left: 20pt;text-indent: -14pt;text-align: justify;"><p style="display: inline;">Experimental results on a series of synthetic dusty images and real dusty images show that the proposed SIDNet achieves a better image dedusting effect than several state-of-the-art im- age restoration methods, especially on real dusty images.</p></li></ol><p class="s18" style="padding-top: 6pt;padding-left: 7pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark36" class="s93">The rest of this paper is organized as follows. </a><a href="#bookmark36" class="s17">Section </a>2 <a href="#bookmark45" class="s93">de- scribes the related works. </a><a href="#bookmark45" class="s17">Section </a>3 <a href="#bookmark52" class="s93">introduces the theory and im- plementation of the proposed method. </a><a href="#bookmark52" class="s17">Section </a>4 <a href="#bookmark78" class="s93">analyzes the ex- periment results, and </a><a href="#bookmark78" class="s17">Section </a>5 <span style=" color: #000;">concludes this paper.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 18pt;text-indent: -10pt;text-align: justify;"><h1 style="display: inline;">Related works</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l3"><li style="padding-left: 22pt;text-indent: -15pt;text-align: justify;"><p class="s21" style="display: inline;">Image formation model learning</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 7pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark83" class="s93">Similar to hazy images, the inﬂuence of dust on an image also depends on image scene depth. Therefore, the relationship of im- age scene depth between dusty and dust-free images should be considered when generating a dusty image. Instead of using the hazy image formation model </a>[17,18] <span style=" color: #000;">directly to depict that of a dusty image, we depict the formation of a dusty image according to its characteristics as,</span></p><p class="s24" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">I<span class="s25">(</span>x<span class="s25">) </span><span class="s26">= </span>J<span class="s25">(</span>x<span class="s25">)</span>d<span class="s25">(</span>x<span class="s25">)</span>l<span class="s25">(</span>x<span class="s25">) </span><span class="s26">+ </span>C<span class="s25">(</span><span class="s27">1 </span><span class="s26">− </span>d<span class="s25">(</span>x<span class="s25">)</span>l<span class="s25">(</span>x<span class="s25">))</span><span class="s28">, </span><span class="s27">(1)</span></p><p class="s21" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><span class="p">where </span>I <span class="p">and </span>J <span class="p">denote a dusty and a dust-free images, respectively. The symbol </span>C <span class="p">denotes a dust color map, i.e., global veiling-light. The symbol </span>l <a href="#bookmark84" class="s93">denotes the image scene depth map obtained by the pre-trained monocular image scene depth estimation method Monodepth2 </a><span class="s18">[19]</span><span class="p">. </span>d <span class="p">denotes the relationship between dusty image scene depth and dust-free image scene depth (brieﬂy called dust- code). To estimate </span>d<span class="s29">(</span>x<span class="s29">) </span><span class="p">accurately, we generate its value by the image scene depth estimation of 20 real dusty images and their corresponding dust-free images obtained by Photoshop in this pa- per.</span></p><p class="s18" style="padding-left: 7pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark83" class="s93">Speciﬁcally, according to the atmospheric scattering model </a>[17,18]<a href="#bookmark38" class="s93">, the relationship between scene depth and transmittance is shown in </a><a href="#bookmark38" class="s17">Eq. </a>(2)<span style=" color: #000;">,</span></p><p class="s24" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;text-align: justify;">t<span class="s25">(</span>x<span class="s25">) </span><span class="s26">= </span>e<span class="s22">−</span><span class="s30">β</span><span class="s13">l</span><span class="s31">(</span><span class="s13">x</span><span class="s31">)</span><span class="s28">, </span><span class="s27">(2)</span></p><p class="s21" style="padding-top: 3pt;padding-left: 7pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><span class="p">where </span>l<span class="s29">(</span>x<span class="s29">) </span><span class="p">is the scene depth and </span>t<span class="s29">(</span>x<span class="s29">) </span><a href="#bookmark38" class="s93">is the transmittance, which describes the proportion of atmospheric light penetrating the aerosol suspended in the air to reach the scene. The higher the transmittance, the less the scene is affected by dust. According to </a><span class="s18">Eq. (2)</span><span class="p">, the impact of dust on scene clarity is logarithmic and inversely proportional to </span>l<span class="s29">(</span>x<span class="s29">) </span><span class="p">that is, the deeper the scene depth,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 151pt;text-align: left;"><span><img width="608" height="202" alt="image" src="Online版本/Image_013.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;text-indent: 0pt;text-align: center;"><a name="bookmark39">Fig. 2. </a><span class="s15">The ﬂow chart of dust-code estimation.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">the smaller the transmittance. Furthermore, when the pixel value of the depth map is smaller, it means that the depth of ﬁeld of the scene is deeper, which is just opposite to the representation of the transmittance map.</p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark84" class="s93">At the same time, since Monodepth2 </a>[19] <span style=" color: #000;">ignores the inﬂu- ence of bad weather on scene visibility during training, the output depth map cannot be used directly to synthesize realistic dusty im- ages. The atmospheric scattering model indicates that there is an exponential relationship between the scene visibility and the scene depth, so the impact of scene visibility on the estimation scene</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">CIELAB color space are as follows,</p><p class="s32" style="padding-left: 22pt;text-indent: 0pt;line-height: 10pt;text-align: left;"> <span class="s33">             </span></p><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">D <span class="s26">= </span>d<span class="s34">2 </span><span class="s26">+ </span>d<span class="s34">2 </span><span class="s28">, </span><span class="s27">(4)</span></p><p class="s36" style="padding-left: 35pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span class="s13">a</span><span class="s35">∗</span> <span class="s37">b</span>∗</p><p class="s32" style="padding-top: 3pt;padding-left: 24pt;text-indent: 0pt;text-align: left;"> <span class="s33">               </span></p><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">M <span class="s26">= </span>m<span class="s34">2 </span><span class="s26">+ </span>m<span class="s34">2 </span><span class="s28">, </span><span class="s27">(5)</span></p><p class="s36" style="padding-left: 40pt;text-indent: 0pt;line-height: 5pt;text-align: left;"><span class="s13">a</span><span class="s35">∗</span> <span class="s37">b</span>∗</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s24" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">D</p><p style="text-indent: 0pt;text-align: left;"><span><img width="11" height="1" alt="image" src="Online版本/Image_014.png"/></span></p><p class="s26" style="text-indent: 0pt;line-height: 8pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">K <span class="s28">, </span><span class="s27">(6)</span></p><p class="s24" style="padding-left: 23pt;text-indent: 0pt;line-height: 5pt;text-align: left;">M</p><p class="s41" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="p">where </span><span class="s21">d</span><span class="s37">a</span><span class="s38">∗ </span><span class="s39">= </span><span class="s40">1 </span>γ<span class="s42">H </span>γ<span class="s42">W</span><span class="s13"> </span><span class="s22">∗</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">depth result of Monodepth2 is also exponential relationship. Since</p><p class="s43" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><span class="s40">    1 </span>γ<span class="s13">H </span>γ<span class="s13">W</span></p><p class="s13" style="padding-left: 13pt;text-indent: 0pt;line-height: 6pt;text-align: left;">H<span class="s23">×</span>W</p><p class="s23" style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;">∗</p><p class="s13" style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p class="s15" style="text-indent: 0pt;line-height: 1pt;text-align: right;">2</p><p class="s21" style="padding-left: 3pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s44">j</span><span class="s23">=</span><span class="s15">1 </span>a <span class="s29">(</span>i<span class="s45">, </span>j<span class="s29">) </span><span class="p">and </span>m<span class="s37">a</span><span class="s38">∗ </span><span class="s39">=</span></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="#bookmark85" class="s93">dusty images have a signiﬁcant impact on the scene visibility </a>[20]<span style=" color: #000;">,</span></p><p class="s13" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">H<span class="s23">×</span>W</p><p class="s13" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p class="s21" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s44">j</span><span class="s23">=</span><span class="s15">1 </span><span class="s29">(</span>a <span class="s29">(</span>i<span class="s45">, </span>j<span class="s29">) </span><span class="s39">− </span>d<span class="s37">a</span><span class="s38">∗ </span><span class="s29">)</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">denote the average chro-</p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;"><a href="#bookmark40" class="s93">we use </a>Eq. (3) <span style=" color: #000;">to estimate the relationship between the scene</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">maticity value and chromaticity center distance of <span class="s21">a</span><span class="s22">∗</span></p><p class="s40" style="text-indent: 0pt;line-height: 1pt;text-align: right;">    1 <span class="s46">H W</span><span class="s13"> </span><span class="s23">∗</span></p><p class="s13" style="text-indent: 0pt;line-height: 7pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p style="text-indent: 0pt;text-align: left;"/><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">depth of clean image and that of dusty image based on mathe-</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">component, respectively. <span class="s21">d</span><span class="s37">b</span><span class="s38">∗ </span><span class="s39">= </span><span class="s44">H</span><span class="s23">×</span><span class="s13">W </span><span class="s41">γ</span></p><p class="s21" style="text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s41">γ </span><span class="s44">j</span><span class="s23">=</span><span class="s15">1 </span>b <span class="s29">(</span>i<span class="s45">, </span>j<span class="s29">) </span><span class="p">and</span></p><p class="s47" style="padding-left: 120pt;text-indent: 0pt;line-height: 0pt;text-align: center;">            <span class="s13">        H W</span></p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 5pt;text-align: left;">matical statistics, which is equivalent to estimating the impact of</p><p class="s21" style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">m<span class="s37">b</span><span class="s38">∗ </span><span class="s39">= </span><span class="s34">1</span></p><p class="s48" style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">γ<span class="s13">i </span><span class="s15">1 </span>γ <span class="s13">j </span><span class="s15">1</span></p><p class="s21" style="text-indent: 0pt;line-height: 6pt;text-align: left;"><span class="s29">(</span>b<span class="s22">∗ </span><span class="s29">(</span>i<span class="s45">, </span>j<span class="s29">) </span><span class="s39">− </span>d<span class="s37">b</span><span class="s38">∗ </span><span class="s29">)</span><span class="s34">2 </span><span class="p">corresponding to that of</span></p><p class="s13" style="padding-left: 122pt;text-indent: 0pt;line-height: 6pt;text-align: center;">H<span class="s23">×</span>W <span class="s23">= =</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">dust on the scene visibility.</p><p class="s24" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark40">y </a><span class="s26">= </span>a <span class="s26">× </span>e<span class="s42">b</span><span class="s49">/</span><span class="s13">x</span><span class="s28">, </span><span class="s27">(3)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark41">where </a><span class="s21">a </span>and <span class="s21">b </span>are the ﬁtting parameters, and they are two arrays with the size of 1 <span class="s39">× </span>255. <span class="s21">x </span>and <span class="s21">y </span>represent the scene depth infor- mation of the clean image and the dusty image, respectively. Addi-</p><p class="s21" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">b<span class="s22">∗ </span><span class="p">component. Generally speaking, an image is judged to have a</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">color cast when the color cast factor <span class="s21">K </span><span class="s39">≥ </span>1<span class="s45">.</span>5.</p><p style="padding-top: 1pt;padding-left: 5pt;text-indent: 11pt;line-height: 78%;text-align: left;"><span class="s50">Color cast correction. </span>Based on the equivalent circle idea, the new corrected <span class="s21">a</span>ˆ<span class="s22">∗</span>, <span class="s21">b</span><span class="s51">ˆ</span><span class="s22">∗ </span>components are generated by,</p><p class="s24" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">a<span class="s27">ˆ</span><span class="s22">∗</span><span class="s23"> </span><span class="s26">= </span>a<span class="s22">∗</span><span class="s23"> </span><span class="s26">− </span>d<span class="s37">a</span><span class="s36">∗ </span><span class="s28">,                                                               </span><span class="s27">(7)</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">tionally, to estimate the relationship between the scene depth in- formation of clean image and that of dusty image depth, we used</p><p class="s52" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">b<span class="s53">ˆ</span><span class="s23">∗</span></p><p class="s26" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">= <span class="s24">b</span><span class="s22">∗</span></p><p class="s26" style="padding-top: 5pt;text-indent: 0pt;text-align: left;">− <span class="s24">d</span><span class="s37">b</span><span class="s36">∗ </span><span class="s28">.                                                               </span><span class="s27">(8)</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark40" class="s93" name="bookmark4">the least square method, i.e., </a><a href="#bookmark40" class="s17">Eq. </a><span style=" color: #0080AC;">(3)</span>, which need two arrays <span class="s21">a </span>and <span class="s21">b </span>to ﬁnd the relationship between two variables <span class="s21">x </span>and <span class="s21">y</span>. More- over, although there are some similarities, the scene depth infor- mation relationships of different clean-dusty image pairs are not completely consistent in practice. Therefore, to reﬂect the general level of dust distribution for more relatively reasonable and natu- ral dusty image synthesis, we average all <span class="s21">a </span>and <span class="s21">b </span>generated from 20 clean-dusty image pairs, and the resulting two arrays with the size of 1 <span class="s39">× </span>255 are named dust-code <span class="s21">d </span><a href="#bookmark37" class="s93">in </a><a href="#bookmark37" class="s17">Eq. </a><span style=" color: #0080AC;">(1)</span><a href="#bookmark39" class="s93">. </a><a href="#bookmark39" class="s17">Fig. </a><span style=" color: #0080AC;">2 </span>gives the ﬂow chart of dust-code estimation.<a name="bookmark5">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: justify;"><p class="s21" style="display: inline;">Traditional image color cast detection and  correction</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 93%;text-align: justify;"><a name="bookmark42"><span class="s50">Color cast detection. </span></a><a href="#bookmark72" class="s93">The method </a><span style=" color: #0080AC;">[4] </span>analyzes the character- istics of an image color distortion in CIELAB color space. CIELAB color space contains three components, that is, lightness compo- nent <span class="s21">L </span>ranging from 0 (dark) to 100 (light), <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>color component ranging from -127 (green) to 128 (red), and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>color component range from -127 (blue) to 128 (yellow). The method further intro- duces the concept of equivalent circle, which takes average chro- maticity values (<span class="s21">d</span><span class="s37">a</span><span class="s38">∗ </span>, <span class="s21">d</span><span class="s37">b</span><span class="s38">∗ </span>) as the circle center coordinate, and chro- maticity center distance <span class="s21">M </span>as the radius. Furthermore, it uses the ratio of <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>jointed average chromaticity value <span class="s21">D </span>to chromaticity center distance <span class="s21">M </span>(i.e., color cast factor <span class="s21">K</span>) to measure the degree of the color cast of an image. The deﬁnitions of <span class="s21">D</span>, <span class="s21">M</span>, and <span class="s21">K </span>in</p><p class="s18" style="padding-top: 1pt;padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark43" class="s93">This technology can correct most images with different color casts. </a><a href="#bookmark43" class="s17">Fig. </a>3 <a href="#bookmark41" class="s93">gives a real dusty image and the restoration result ac- cording to </a><a href="#bookmark41" class="s17">Eqs. </a>(7) <span style=" color: #000;">and (8), and their corresponding equivalent cir- cles. After color cast correction, the average chromaticity values of </span><span class="s21">a</span><span class="s22">∗</span><span style=" color: #000;">, </span><span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span><span style=" color: #000;">components, and color cast factor </span><span class="s21">K </span><a href="#bookmark43" class="s93">are obviously smaller, shown in </a><a href="#bookmark43" class="s17">Fig. </a>3<a href="#bookmark43" class="s93">(b) and (d). However, such color cast correction technologies exist a common problem that the color saturation of the restoration results are still relatively low, shown in </a><a href="#bookmark43" class="s17">Fig. </a>3<span style=" color: #000;">(c).</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><p class="s21" style="display: inline;">Deep learning-based image restoration</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark77" class="s93">Recently, convolution neural networks (CNNs) have been widely and successfully applied in restoring an image under bad weather. Owing to the available and publicly related synthetic datasets for network training </a>[11,13]<a href="#bookmark73" class="s93">, there are many deep learning-based methods focusing on hazy images </a>[6–8]<a href="#bookmark74" class="s93">, rainy images </a>[9,10] <a href="#bookmark77" class="s93">and underwater images restoration </a>[11]<a href="#bookmark86" class="s93">. Deep learning-based image restoration networks can be divided into two categories. One cate- gory network automatically estimates a transmission map and the atmospheric light value. Following this, it generates image dehaz- ing results according to the atmospheric scattering model, such as </a>[21]<a href="#bookmark75" class="s93">. The other category is end-to-end networks </a>[7,16]<span style=" color: #000;">, which di- rectly generate the image dehazing result by network learning and do not rely on the atmospheric scattering model. However, due to the lack of dusty image datasets, image dedusting technology</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 126pt;text-align: left;"><span><img width="608" height="168" alt="image" src="Online版本/Image_015.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;text-indent: 0pt;text-align: center;"><a name="bookmark43">Fig. 3. </a><span class="s15">Color cast correction on a real dusty image.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 191pt;text-align: left;"><span><img width="608" height="255" alt="image" src="Online版本/Image_016.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="text-indent: 0pt;text-align: center;"><a name="bookmark44">Fig. 4. </a><span class="s15">Network structure of the proposed SIDNet.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark72" class="s93">still stays in various traditional color correction ways </a>[4,5]<span style=" color: #000;">, whose restoration image saturation is still not enough. Furthermore, most of the existing deep learning-based methods have a common prob- lem that they generate good effect on a synthetic dataset. How-</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">expressed as,</p><p class="s24" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">x<span class="s42">i</span><span class="s23">+</span><span class="s15">1 </span><span class="s26">= </span><span class="s25">(</span><span class="s54">↓ </span>y<span class="s42">i </span><span class="s25">) </span>x<span class="s42">i </span><span class="s28">, </span><span class="s27">(9)</span></p><p class="s15" style="padding-left: 9pt;text-indent: 0pt;line-height: 6pt;text-align: left;">2 1 <span class="s55">EB</span><span class="s56"> </span>2</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;">ever, they show poor effect on real scenes. Inspired by the wide</p><p class="s52" style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;">x<span class="s13">i</span><span class="s23">+</span><span class="s15">1</span></p><p class="s13" style="padding-left: 5pt;text-indent: 0pt;line-height: 2pt;text-align: left;">i i i</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark6">application of CNNs, we presented a network with color cast cor- rection for single image dedusting and constructed a new dusty image dataset for the network training.</a></p><p class="s25" style="padding-left: 9pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s57">3 </span><span class="s26">= </span>(<span class="s54">↓↓ </span><span class="s24">y</span><span class="s58">1 </span>) <span class="s56">EB </span>(<span class="s54">↓ </span><span class="s24">y</span><span class="s58">2 </span>) <span class="s56">EB </span><span class="s24">x</span><span class="s58">3</span><span class="s28">. </span><span class="s27">(10)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;">Similarly, during the upsample processing, the input of the <span class="s29">(</span><span class="s21">i </span><span class="s39">+ </span>1<span class="s29">)</span>-th DustAre module in the second and the ﬁrst row can be expressed as,</p></li></ol></li><li style="padding-top: 6pt;padding-left: 16pt;text-indent: -10pt;line-height: 5pt;text-align: left;"><h1 style="display: inline;">The proposed method</h1><p class="s24" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark45">x</a><span class="s42">i</span><span class="s23">+</span><span class="s15">1 </span><span class="s26">= </span><span class="s25">(</span><span class="s54">↑ </span>y<span class="s42">i</span><span class="s13"> </span><span class="s25">)</span></p><p class="s24" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">x<span class="s42">i </span><span class="s28">, </span><span class="s27">(11)</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;line-height: 7pt;text-align: left;">To alleviate the common problem mentioned above, we pre-</p><p class="s15" style="padding-left: 17pt;text-indent: 0pt;line-height: 7pt;text-align: left;">2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-top: 4pt;padding-left: 17pt;text-indent: 0pt;line-height: 3pt;text-align: left;">i<span class="s23">+</span><span class="s15">1</span></p><p class="s15" style="padding-left: 17pt;text-indent: 0pt;line-height: 6pt;text-align: left;">3 <span class="s59">EB</span><span class="s56"> </span>2</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 23pt;text-indent: 0pt;line-height: 2pt;text-align: left;">i i i</p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark7">sented an effective image dedusting network with color cast cor- rection (SIDNet). The following subsection ﬁrst introduces the net- work structure of the SIDNet, and then describes its important details, including dust-aware representation extraction (DustAre) module, color cast correction, and loss function.</a><a name="bookmark8">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l4"><ol id="l5"><li style="padding-left: 20pt;text-indent: -15pt;text-align: justify;"><p class="s21" style="display: inline;">Network structure</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark44" class="s93">The proposed network uses several dust-aware representation extraction (DustAre) modules to downsample a dusty image for ex- tracting image feature maps and then upsample the image feature maps to generate the ﬁnal dedusting result. As shown in </a><a href="#bookmark44" class="s17">Fig. </a>4<span style=" color: #000;">, the height and width of the network are three and six, respec-</span></p><p class="s25" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;"><span class="s24">x</span><span class="s57">1 </span><span class="s26">= </span>(<span class="s54">↑↑ </span><span class="s24">y</span><span class="s58">3 </span>) <span class="s56">EB </span>(<span class="s54">↑ </span><span class="s24">y</span><span class="s58">2 </span>) <span class="s56">EB </span><span class="s24">x</span><span class="s58">1</span><span class="s28">, </span><span class="s27">(12)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 86%;text-align: left;">where the operator <span class="s60">↓ </span>and <span class="s60">↑ </span>denote a downsample and a upsample operation, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><p class="s21" style="display: inline;">Dust-aware representation extraction module</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark82" class="s93">In order to remove the dust from an image while preserving image details, estimations of both global veiling-light and local spectral information are important. Inspired by the existing dehaz- ing network </a><span style=" color: #0080AC;">[16]</span><a href="#bookmark46" class="s93">, we use the dust-aware representation extraction module to extract the representations of them, as shown in </a><a href="#bookmark46" class="s17">Fig. </a><span style=" color: #0080AC;">5</span>. The dust-aware representation extraction module mainly contains two branches. The ﬁrst branch uses a 1 <span class="s39">× </span>1 <span class="s39">× </span>2 matrix to encode</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">tively. Speciﬁcally, we use <span class="s21">x</span><span class="s42">i </span>, <span class="s21">y</span><span class="s42">i</span></p><p style="padding-left: 3pt;text-indent: 0pt;line-height: 6pt;text-align: left;">to denote the input and output</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 6pt;text-align: left;">the input and generate the global veiling-light representation (i.e.,</p><p class="s13" style="padding-left: 113pt;text-indent: 0pt;line-height: 5pt;text-align: left;">r r</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">of the <span class="s21">i</span>-th DustAre module in the <span class="s21">r</span>-th row of the network, respec-</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">tively. Then, during the downsample processing, the input of the</p><p class="s29" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">(<span class="s21">i </span><span class="s39">+ </span><span class="p">1</span>)<span class="p">-th DustAre module in the second and the third row can be</span></p><p class="s25" style="padding-left: 5pt;text-indent: 0pt;line-height: 80%;text-align: left;">β<span class="s42">V</span><span class="s13"> </span><span class="s45">, </span>γ <span class="s42">V</span><span class="s13"> </span><span class="p">), and a </span><span class="s21">H </span><span class="s39">× </span><span class="s21">W </span><span class="s39">× </span><span class="p">2 matrix to generate the local spectral in- formation representation (i.e., </span>β<span class="s42">S</span><span class="s45">, </span>γ <span class="s42">S</span><span class="s13"> </span><span class="p">). The second branch is used</span></p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">to normalize two pair representations with the input <span class="s21">x </span>according</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 231pt;text-align: left;"><span><img width="608" height="308" alt="image" src="Online版本/Image_017.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 170pt;text-indent: 0pt;text-align: left;"><a name="bookmark46">Fig. 5. </a><span class="s15">Dust-aware representation extraction (DustAre) module.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark47" class="s93">to </a><a href="#bookmark47" class="s17">Eqs. </a><span style=" color: #0080AC;">(13) </span>and (14) (deﬁned as <span class="s21">V </span>and <span class="s21">S</span>, respectively), and gener- ate a dust-aware map (<span class="s21">A</span>) by going through an instance normaliza- tion layer followed by a sigmoid layer at the same time. Following this, it fuses <span class="s21">V </span>and <span class="s21">S </span>based on <span class="s21">A </span>to generate the output y of the DustAre module. In detail, for the input <span class="s21">x </span>of the DustAre module,</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">to more realistic dusty scenes. Speciﬁcally, it thinks those ground- truth images with color cast factor <span class="s21">K </span><span class="s39">≥ </span>1<span class="s45">.</span>5 are more likely to have color cast themselves. Therefore, we correct <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>components of these images in CIELAB color space, i.e., ﬁnal training ground truth image <span class="s21">y</span><span class="s51">ˆ</span>ˆ is generated by the following expression,</p><p class="s21" style="padding-left: 34pt;text-indent: -29pt;line-height: 11pt;text-align: left;">V <span class="p">, </span>S <span class="p">and output </span>y <span class="p">are expressed as,</span></p><p class="s61" style="padding-left: 34pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a name="bookmark47">x </a><span class="s62">− </span><span class="s63">μ</span></p><p class="s24" style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">y<span class="s64">ˆ</span><span class="s27">ˆ </span><span class="s26">=</span></p><p class="s24" style="text-indent: 0pt;line-height: 18pt;text-align: left;"><span class="s65">．</span>cor<span class="s25">(</span>y<span class="s27">ˆ</span><span class="s25">)</span><span class="s28">, </span>K</p><p class="s27" style="padding-top: 5pt;text-indent: 0pt;line-height: 13pt;text-align: left;"><span class="s26">≥ </span>1<span class="s28">.</span>5</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s28" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">, <span class="s27">(16)</span></p><p class="s26" style="text-indent: 0pt;line-height: 8pt;text-align: left;">=</p><p style="text-indent: 0pt;text-align: left;"/><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 7pt;text-align: left;">V <span class="s66">β</span><span class="s42">V</span></p><p class="s66" style="text-indent: 0pt;line-height: 9pt;text-align: right;">σ</p><p class="s26" style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">+ <span class="s66">γ </span><span class="s42">V </span><span class="s28">, </span><span class="s27">(13)</span></p><p class="s27" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;"><span class="s24">y</span>ˆ<span class="s28">, </span>otherwise</p><p class="s24" style="padding-top: 9pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">S <span class="s26">= </span><span class="s66">β</span><span class="s42">S</span></p><p class="s61" style="padding-top: 3pt;padding-left: 7pt;text-indent: -7pt;line-height: 12pt;text-align: left;">x <span class="s62">− </span><span class="s63">μ </span><span class="s66">σ</span></p><p class="s26" style="padding-top: 9pt;padding-left: 1pt;text-indent: 0pt;text-align: left;">+ <span class="s66">γ </span><span class="s42">S</span><span class="s28">, </span><span class="s27">(14)</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">where <span class="s21">y</span>ˆ denotes an original ground-truth image, <span class="s21">cor </span><a href="#bookmark42" class="s93">denotes the color cast correction operation mentioned in </a><span style=" color: #0080AC;">Section 2.2</span><a href="#bookmark48" class="s93">. </a><span style=" color: #0080AC;">Fig. 6 </span>gives comparisons of original training ground truth images and their corresponding images after color correction. It can be ob-</p><p class="s24" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">y <span class="s26">= </span>V <span class="s56">0 </span>A <span class="s26">+ </span>S <span class="s56">0 </span><span class="s25">(</span><span class="s27">1 </span><span class="s26">− </span>A<span class="s25">)</span><span class="s28">, </span><span class="s27">(15)</span></p><p style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark9">where </a><span class="s25">μ </span>and <span class="s25">σ </span>are denote the mean and standard deviation of <span class="s21">x</span>, respectively.<a name="bookmark10">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><p class="s21" style="display: inline;">Color cast correction</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark87" class="s93">In fact, most of the existing deep learning-based image restora- tion methods have a common problem that they achieve good ef- fect on synthetic datasets while showing poor effect on real im- ages. One of the main reasons is that the ﬁnal image restoration results are overly dependent on the training dataset and even pro- duce unnatural colors due to the color cast of the training dataset itself. On the one hand, the training set in the paper is synthesized based on two standard image datasets with scene depth maps (the outdoor clean image dataset Cityscape </a>[22] <a href="#bookmark88" class="s93">and the indoor clean image dataset NYU-depth </a>[23]<span style=" color: #000;">). The Cityscape is captured from dif- ferent urban scenes during driving with low brightness, and the NYU-depth is captured from indoor scenes that some images of this dataset have color cast under the inﬂuence of light. On the other hand, since images captured in sand-dust storm weather are often greatly affected by image scene brightness, improving the brightness of dusty images is also an essential step of the image dedusting task. Therefore, in order to make the model better ap- plied to real dusty scenes with higher brightness and more natu- ral scene color, the SIDNet uses color cast detection and correction for ground-truth images from the training dataset. Additionally, due to the problem of single scene of the two datasets, this pre- processing operation can enrich the scenes at a certain to adapt</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">served that the brightness of the adjusted images is signiﬁcantly improved, and the colors are more natural.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: justify;"><p class="s21" style="display: inline;">Loss function</p></li></ol></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 17pt;text-indent: 0pt;text-align: left;">The proposed SIDNet deﬁnes its loss function as,</p><p class="s67" style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: justify;">L <span class="s26">= </span><span class="s66">λ</span><span class="s44">ab</span>L<span class="s44">ab </span><span class="s26">+ </span><span class="s66">λ</span><span class="s37">L</span><span class="s15">1</span>L<span class="s37">L</span><span class="s15">1 </span><span class="s26">+ </span><span class="s66">λ</span><span class="s37">per </span>L<span class="s37">per </span><span class="s28">, </span><span class="s27">(17)</span></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">where <span class="s25">λ</span><span class="s44">ab</span>, <span class="s25">λ</span><span class="s37">L</span><span class="s15">1 </span>and<span class="s25">λ</span><span class="s37">per</span><span class="s13"> </span>are set to 0.3, 1.2 and 0.4, respectively. The ﬁrst item <span class="s68">L</span><span class="s44">ab</span><span class="s13"> </span>stems from the observation of dusty images and dust-free images. Due to a dusty image having serious color casts, the average <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span><a href="#bookmark49" class="s93">chromaticity values of a dusty image in CIELAB color space are usually larger than that of a dust-free image. </a><a href="#bookmark49" class="s17">Fig. </a><span style=" color: #0080AC;">7 </span><a href="#bookmark49" class="s93">exhibits two similar real outdoor scenes with and without dust as samples to validate this observation. </a><a href="#bookmark49" class="s17">Fig. </a><span style=" color: #0080AC;">7</span>(a)-(d) show a real dusty image, its <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>component in CIELAB color space, its <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span><a href="#bookmark49" class="s93">component in CIELAB, and its equivalent circle, respectively. Similarly, </a><a href="#bookmark49" class="s17">Fig. </a><span style=" color: #0080AC;">7</span>(e)-</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 92%;text-align: justify;">(h) show a real dust-free image and its corresponding <span class="s21">a</span><span class="s22">∗ </span>compo- nent, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span><a href="#bookmark49" class="s93">component, and equivalent circle, respectively. From </a><a href="#bookmark49" class="s17">Fig. </a><span style=" color: #0080AC;">7</span>, one can be observed that the <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>components of a dusty im- age are blurred and brighter than those of a dust-free image, which indicates the average <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span><a href="#bookmark49" class="s93">chromaticity values of a dusty image are larger than those of a dust-free image. This is consistent with the data given in </a><a href="#bookmark49" class="s17">Fig. </a><span style=" color: #0080AC;">7</span>(d) and (h). To further validate this obser- vation, we calculate the <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>average chromaticity values on 50 real dusty and dust-free images. Experimental data shows that 50 real dusty images have an average <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>chromaticity values of 13.53 and 35.69, respectively. 50 dust-free images have an average</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 184pt;text-align: left;"><span><img width="560" height="245" alt="image" src="Online版本/Image_018.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark48">Fig. 6. </a><span class="s15">Comparisons of original training ground-truth images and their corresponding images after color correction. (a) three original ground-truth images from Cityscapes,</span></h2><p class="s15" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">(b) after color cast correction corresponding to (a); (c) three original ground-truth images from NYU-depth, (d) after color cast corresponding to (c).</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 236pt;text-align: left;"><span><img width="608" height="315" alt="image" src="Online版本/Image_019.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="text-indent: 0pt;text-align: center;"><a name="bookmark49"><span class="h2">Fig. 7. </span></a>The <span class="s13">a</span><span class="s19">∗</span>, <span class="s13">b</span><span class="s19">∗</span><span class="s69"> </span>components on a pair of similar scenes with and without dust.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s21" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">a<span class="s22">∗</span><span class="p">, </span>b<span class="s22">∗</span><span class="s23"> </span><span class="p">chromaticity values of 0.28 and 2.99, respectively. Following these data, one can conclude that a dusty image usually has large</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: left;">The second item is smooth <span class="s21">L</span>1 loss termed as <span class="s68">L</span><span class="s37">L</span><span class="s15">1</span>. The <span class="s68">L</span><span class="s37">L</span><span class="s15">1 </span><a href="#bookmark89" class="s93">can prevent potential gradient explosion </a><span style=" color: #0080AC;">[24]</span>, which is deﬁned as,</p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 12pt;text-align: left;">average <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗ </span>chromaticity values in CIELAB color space. Therefore, <span class="s58">3</span><span class="s15"> </span><span class="s13">N</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;">we deduce that the smaller the average <span class="s21">a</span><span class="s22">∗</span>, <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>chromaticity values</p><p style="padding-left: 5pt;text-indent: 0pt;text-align: left;">of an image dedusting result, the better the image dedusting ef- fect. Motivated by the observations above mentioned, we designed</p><p class="s67" style="padding-left: 5pt;text-indent: 0pt;text-align: left;">L<span class="s37">L</span><span class="s15">1 </span><span class="s26">=</span></p><p class="s70" style="text-indent: 0pt;line-height: 4pt;text-align: left;">「 「</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="text-indent: 0pt;text-align: left;">c<span class="s23">=</span><span class="s15">1 </span>i<span class="s23">=</span><span class="s15">1</span></p><p class="s66" style="text-indent: 0pt;line-height: 13pt;text-align: left;">α<span class="s25">(</span><span class="s24">y</span><span class="s27">ˆ</span><span class="s37">c</span></p><p class="s25" style="text-indent: 0pt;text-align: left;">(<span class="s24">i</span>) <span class="s26">− </span><span class="s24">y</span><span class="s37">c</span>(<span class="s24">i</span>))<span class="s28">, </span><span class="s27">(19)</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">the ﬁrst loss item <span class="s68">L</span><span class="s44">ab</span><span class="s13"> </span>as,</p><p class="s25" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><span class="s66">α</span>(<span class="s24">k</span>) <span class="s26">=</span></p><p class="s24" style="padding-left: 3pt;text-indent: 0pt;line-height: 9pt;text-align: center;"><span class="s65">．</span><span class="s27">0</span><span class="s28">.</span><span class="s27">5</span>k<span class="s34">2</span><span class="s28">, </span>if <span class="s54">|</span>k<span class="s54">| </span><span class="s28">&lt; </span><span class="s27">1</span></p><p class="s54" style="padding-left: 7pt;text-indent: 0pt;line-height: 13pt;text-align: center;">|<span class="s24">k</span>| <span class="s26">− </span><span class="s27">0</span><span class="s28">.</span><span class="s27">5</span><span class="s28">, </span><span class="s24">otherwise</span></p><p class="s28" style="padding-top: 3pt;text-indent: 0pt;text-align: left;">. <span class="s27">(20)</span></p><p class="s71" style="padding-top: 6pt;padding-left: 35pt;text-indent: 0pt;line-height: 2pt;text-align: left;">1 <span class="s13">N </span>1<span class="s27"> </span><span class="s13">N</span></p><p class="s71" style="padding-top: 6pt;padding-left: 35pt;text-indent: 0pt;line-height: 2pt;text-align: left;">1 <span class="s13">N </span>1<span class="s27"> </span><span class="s13">N</span></p><p style="padding-top: 4pt;padding-left: 35pt;text-indent: 0pt;line-height: 4pt;text-align: left;">The ﬁnal item, perceptual loss <span class="s68">L</span><span class="s37">per </span>, is used to encourage de-</p><p class="s70" style="padding-left: 34pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s72">   </span><span class="s73"> </span>「  「 <span class="s74">ˆ</span></p><p class="s70" style="padding-left: 15pt;text-indent: 0pt;line-height: 7pt;text-align: left;"><span class="s75">2 </span>「 「 <span class="s74">ˆ</span><span class="s27"> </span><span class="s75">2</span></p><p class="s67" style="padding-left: 5pt;text-indent: 0pt;line-height: 15pt;text-align: left;">L<span class="s44">ab </span><span class="s26">= </span><span class="s25">( </span><span class="s76">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 1pt;text-indent: 0pt;text-align: left;"><a name="bookmark50">i</a><span class="s23">=</span><span class="s15">1</span></p><p class="s24" style="text-indent: 0pt;line-height: 15pt;text-align: left;">y<span class="s37">a</span><span class="s25">(</span>i<span class="s25">) </span><span class="s26">− </span><span class="s76">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p class="s24" style="text-indent: 0pt;line-height: 10pt;text-align: left;">y<span class="s27">ˆ</span><span class="s37">a</span><span class="s13"> </span><span class="s25">(</span>i<span class="s25">))</span></p><p class="s26" style="padding-left: 3pt;text-indent: 0pt;line-height: 15pt;text-align: left;">+ <span class="s25">( </span><span class="s76">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p class="s24" style="text-indent: 0pt;line-height: 15pt;text-align: left;">y<span class="s44">b</span><span class="s25">(</span>i<span class="s25">) </span><span class="s26">− </span><span class="s76">N</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s13" style="padding-left: 1pt;text-indent: 0pt;text-align: left;">i<span class="s23">=</span><span class="s15">1</span></p><p class="s24" style="text-indent: 0pt;line-height: 11pt;text-align: left;">y<span class="s27">ˆ</span><span class="s44">b </span><span class="s25">(</span>i<span class="s25">)) </span><span class="s28">,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s27" style="padding-left: 16pt;text-indent: 0pt;text-align: left;">(18)</p><p class="s18" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a href="#bookmark90" class="s93">dusting image to be represented with a similar feature in the back- bone network, such as VGG19 pre-trained </a>[25] <span style=" color: #000;">in this work, and its deﬁnition is,</span></p><p class="s15" style="text-indent: 0pt;line-height: 6pt;text-align: left;">3</p><p style="text-indent: 0pt;text-align: left;"/><p class="s77" style="padding-left: 32pt;text-indent: 0pt;line-height: 13pt;text-align: left;">「<span class="s70"> </span><span class="s78">1     </span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">where <span class="s21">N </span>is the total number of image pixels. <span class="s21">y</span><span class="s37">a</span><span class="s13"> </span>and <span class="s21">y</span><span class="s44">b</span><span class="s13"> </span>denote the</p><p class="s79" style="padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;">L<span class="s13">per </span><span class="s80">=</span></p><p class="s24" style="padding-top: 5pt;padding-left: 5pt;text-indent: 0pt;line-height: 5pt;text-align: left;">C H W</p><p class="s25" style="padding-left: 3pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><span class="s54">1 </span><span class="s66">φ</span>(<span class="s24">y</span>) <span class="s26">− </span><span class="s66">φ</span>(<span class="s24">y</span><span class="s27">ˆ</span>) <span class="s54">1</span><span class="s28">, </span><span class="s27">(21)</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 83%;text-align: justify;"><span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>color components in CIELAB color space of dedusting re- sult <span class="s21">y</span>, respectively. <span class="s21">y</span><span class="s51">ˆ</span>ˆ<span class="s37">a </span>and <span class="s21">y</span><span class="s51">ˆ</span>ˆ<span class="s44">b </span>denote the <span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>color compo- nents in CIELAB color space of corresponding dust-free image (i.e., ground-truth <span class="s21">y</span>ˆ) after color cast correction <span class="s21">y</span><span class="s51">ˆ</span>ˆ, respectively.</p><p class="s81" style="padding-left: 33pt;text-indent: 0pt;line-height: 10pt;text-align: left;">i<span class="s23">=</span><span class="s15">1 </span><span class="s13">i i i</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">where <span class="s21">H</span>, <span class="s21">W </span>, and <span class="s21">C </span>denote height, weight, and channel of the fea- ture map in the <span class="s21">i</span>-th layer of the network, respectively. <span class="s25">φ</span><span class="s44">i</span><span class="s29">(</span><span class="s21">y</span><span class="s29">) </span>and <span class="s25">φ</span><span class="s44">i</span><span class="s29">(</span><span class="s21">y</span>ˆ<span class="s29">) </span>denote the output feature map in the <span class="s21">i</span>-th layer of <span class="s21">y </span>and <span class="s21">y</span>ˆ,</p><h2 style="padding-top: 3pt;padding-left: 47pt;text-indent: 0pt;text-align: left;"><a name="bookmark51">Table 1</a></h2><p class="s15" style="padding-left: 47pt;text-indent: 0pt;text-align: left;">The training details of the compared methods.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:47.572pt" cellspacing="0"><tr style="height:14pt"><td style="width:53pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Methods</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">LPNet </a><a href="#bookmark74" class="s83">[9]</a></p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="#bookmark79" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">RGNet </a><a href="#bookmark79" class="s83">[10]</a></p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">AODNet </a><a href="#bookmark75" class="s83">[7]</a></p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFANet </a><a href="#bookmark76" class="s83">[8]</a></p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">Wang et al. </a><a href="#bookmark72" class="s83">[4]</a></p></td><td style="width:53pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark82" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">HardGAN </a><a href="#bookmark82" class="s83">[16]</a></p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFNet </a><a href="#bookmark80" class="s83">[12]</a></p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SIDNet</p></td></tr><tr style="height:12pt"><td style="width:53pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Frame</p></td><td style="width:47pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">TensorFlow</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">TensorFlow</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PyTorch</p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PyTorch</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">MatLab</p></td><td style="width:53pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">PyTorch</p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PyTorch</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">PyTorch</p></td></tr><tr style="height:9pt"><td style="width:53pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Task</p></td><td style="width:47pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">deraining</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">deraining</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dehazing</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dehazing</p></td><td style="width:57pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dedusting</p></td><td style="width:53pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dehazing</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dedusting</p></td><td style="width:44pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">dedusting</p></td></tr><tr style="height:9pt"><td style="width:53pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Epochs</p></td><td style="width:47pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:57pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">/</p></td><td style="width:53pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td><td style="width:44pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">200</p></td></tr><tr style="height:9pt"><td style="width:53pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Learning_rate</p></td><td style="width:47pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-3</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-3</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-4</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-4</p></td><td style="width:57pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">/</p></td><td style="width:53pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-4</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-4</p></td><td style="width:44pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">1e-4</p></td></tr><tr style="height:9pt"><td style="width:53pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Batch_size</p></td><td style="width:47pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:57pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">/</p></td><td style="width:53pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:45pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:44pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td></tr><tr style="height:12pt"><td style="width:53pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Image_size</p></td><td style="width:47pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:45pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:57pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:53pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:45pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td><td style="width:44pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">512 <span class="s84">× </span>256</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark11">respectively. </a><span class="s21">y </span>and <span class="s21">y</span>ˆ denote dedusting result and corresponding dust-free image, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 16pt;text-indent: -10pt;text-align: left;"><h1 style="display: inline;"><a name="bookmark52">Experiments</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark74" class="s93" name="bookmark13">To quantitative and qualitative evaluation image dedusting per- formance, we compared the SIDNet with seven state-of-the-art im- age restoration methods, i.e., LPNet </a>[9]<a href="#bookmark79" class="s93">, RGNet </a>[10]<a href="#bookmark75" class="s93">, AOD-Net </a>[7]<a href="#bookmark76" class="s93">, FFANet </a>[8]<a href="#bookmark72" class="s93">, Wang et al. </a>[4]<a href="#bookmark82" class="s93">, HardGAN </a>[16]<a href="#bookmark80" class="s93">, FFNet </a>[12]<a href="#bookmark51" class="s93">. Among the seven methods, where LPNet, RGNet focus on image deraining, AOD-Net, FFANet, and HardGAN focus on image dehazing, while the other two methods concentrate on image dedusting. It is worth noting that all the compared methods are deep-learning based, ex- cept the method of Wang et al.. Speciﬁcally, the training details of all the compared methods are shown in </a><a href="#bookmark51" class="s17">Table </a>1<span style=" color: #000;">, where the learn- ing rate of each method is following their original setting.</span></p><p style="padding-left: 5pt;text-indent: 11pt;text-align: justify;"><a name="bookmark12">This section ﬁrstly describes the dusty image datasets for net- work training and evaluation metrics for image dedusting capabil- ity. Then, it presents quantitative and qualitative evaluation results of the eight methods on synthetic and real dusty images. Next, it veriﬁes the signiﬁcance of image dedusting on image edge extrac- tion and local keypoints matching. Finally, it analyzes the inﬂu- ence of several important designs in the proposed SIDNet, includ- ing DustAre module, color cast correction, and loss function item.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l6"><li style="padding-left: 20pt;text-indent: -15pt;text-align: left;"><p class="s21" style="display: inline;">Datasets and evaluation  metrics</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark91" class="s93">There are few publicly available dusty image datasets which is diﬃcult to collect in practice. Synthetic datasets are widely used to train models in the image enhancement ﬁelds. For example, Zhang et al. </a>[26] <a href="#bookmark92" class="s93">used Photoshop to assist in image deraining task, and Li et al. </a>[27] <a href="#bookmark87" class="s93">selected 3D movie pre-processing to provide outdoor stereo image pairs for haze synthesis. Therefore, we also used our synthetic dusty image dataset for several network training. In de- tail, the outdoor dusty images are synthesized from the Cityscapes dataset </a>[22]<a href="#bookmark88" class="s93">, while the indoor dusty images are synthesized from the NYU-depth dataset </a>[23]<a href="#bookmark53" class="s93">. The dusty image dataset is composed of three subsets: outdoor synthetic training set, indoor synthetic training set, and synthetic testing set. The outdoor and indoor syn- thetic training set contains 800 (200 original dust-free images and their 600 synthetic images dusty images) and 2352 (588 original dust-free images and their 1764 synthetic dusty images) images, respectively. The synthetic testing set is composed of 268 outdoor synthetic dusty images (67 original dust-free images and 201 syn- thetic dusty images) and 308 indoor synthetic dusty images (77 original dust-free images and 231 synthetic dusty images). In addi- tion, to verify the effectiveness of the proposed SIDNet on the real dusty scene, we chose 55 representative real dusty images under different scenes as testing samples. </a><a href="#bookmark53" class="s17">Fig. </a>8 <span style=" color: #000;">exhibits some example images from our synthetic dusty dataset and real dusty images. It can be observed that the RGB color histograms of synthetic dusty images conform to the distribution characteristics of real dusty im- ages, i.e., the three-color distributions of a dusty image are rela- tively separated and concentrated in the centralized sub-interval of the histogram.</span></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark93" class="s93">To quantitatively compare dedusting performances of differ- ent methods, we use PSNR (peak signal to noise ratio) and SSIM (structural similarity) </a>[28] <a href="#bookmark94" class="s93">to evaluate dedusting results of syn- thetic dusty images with corresponding reference images and use spatial-spectral entropy-based quality (SSEQ) </a>[29] <a href="#bookmark95" class="s93">and image en- tropy (E) </a>[30] <span style=" color: #000;">to evaluate dedusting results of real dusty images without corresponding reference images. Among all the metrics, higher PSNR, SSIM, and E values indicate better dedusting results and SSEQ value range between 0 (best) to 100 (worst). To make the correlation consistent with PSNR and SSIM metrics, we reverse the SSEQ value in this paper.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-top: 5pt;padding-left: 22pt;text-indent: -16pt;text-align: left;"><p class="s21" style="display: inline;">Quantitative evaluation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark54" class="s93">To make a quantitative comparison of different methods, </a><a href="#bookmark54" class="s17">Tables </a>2<a href="#bookmark56" class="s93">–</a>4 <a href="#bookmark54" class="s93">list average PSNR, SSIM, and SSEQ values of different methods on synthetic and real dusty images. The best and second- best quantitative data of the three tables were marked in red and blue, respectively. Speciﬁcally, </a><a href="#bookmark54" class="s17">Tables </a>2 <a href="#bookmark55" class="s93">and </a>3 <a href="#bookmark56" class="s93">list average PSNR and SSIM values of outdoor and indoor synthetic dusty image dedust- ing results obtained by different methods, respectively. </a><a href="#bookmark56" class="s17">Table </a>4 <span style=" color: #000;">lists SSEQ values of eight real dusty images dedusting results, as well as their corresponding average SSEQ values obtained by different methods.</span></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark54" class="s17">Table </a>2 <a href="#bookmark55" class="s93">shows that the proposed SIDNet obtained the high- est average PSNR (32.54) and the highest SSIM (0.9294) val- ues on the dedusting results of outdoor synthetic dusty images. HardGAN and FFNet exhibit their effective dedusting performance with higher average PSNR and SSIM values than other reference methods. FFANet, AODNet, LPNet, and RGNet display intermediate dedusting performance with middle average PSNR and SSIM val- ues. Wang et al. exhibit their poor dedusting performance with the lowest average PSNR and SSIM values. </a><a href="#bookmark55" class="s17">Table </a>3 <span style=" color: #000;">demonstrates that the SIDNet achieves the highest average PSNR (32.82) and SSIM (0.9432) values on the dedusting results of indoor syn- thetic dusty images indicating better dedusting performances than other reference methods. HardGAN gains the second-highest PSNR and the second-highest SSIM values. FFNet achieves the third- highest PSNR and SSIM values, while other reference methods ob- tain lower PSNR and SSIM values showing the worse dedusting performances.</span></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;text-align: justify;"><a href="#bookmark56" class="s93">To further validate the performance of image dedusting on real dusty images, eight representative real dusty images were selected as testing samples. These images depict several common dusty scenes, such as suburbs, roads, city, and outside of a window. Quantitative performances in </a><a href="#bookmark56" class="s17">Table </a>4 <span style=" color: #000;">exhibit that the proposed SIDNet obtains the highest SSEQ values on most of the images, ex- cluding the ﬁfth and the eighth images, and achieves the second- highest SSEQ value on the ﬁrst image. Furthermore, the SIDNet ex- hibits the best dedusting performance on the real dusty images with the highest average SSEQ value of 69.66. Among the seven counterparts, AODNet, RGNet, and FFANet obtain the highest SSEQ values on the ﬁrst, ﬁfth and last image, respectively. The method of Wang et al. exhibits better dedusting performances with the second-highest SSEQ values on the third to seventh images and the</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 244pt;text-align: left;"><span><img width="608" height="326" alt="image" src="Online版本/Image_020.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 131pt;text-indent: 0pt;text-align: left;"><a name="bookmark53">Fig. 8. </a><span class="s15">Example images from our synthetic dusty image dataset and 9 real dusty images.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 67pt;text-indent: 0pt;text-align: left;"><a name="bookmark54">Table 2</a></h2><p class="s15" style="padding-left: 67pt;text-indent: 0pt;line-height: 106%;text-align: left;">Average PSNR and SSIM values of outdoor synthetic dusty image dedusting results obtained by different methods. The data marked in red and blue indicate the best and the second-best quantitative evaluation results, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:67.228pt" cellspacing="0"><tr style="height:14pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Methods</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">LPNet </a><a href="#bookmark74" class="s83">[9]</a></p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark79" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">RGNet </a><a href="#bookmark79" class="s83">[10]</a></p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">AODNet </a><a href="#bookmark75" class="s83">[7]</a></p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFANet </a><a href="#bookmark76" class="s83">[8]</a></p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">Wang et al. </a><a href="#bookmark72" class="s83">[4]</a></p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark82" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">HardGAN </a><a href="#bookmark82" class="s83">[16]</a></p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFNet </a><a href="#bookmark80" class="s83">[12]</a></p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SIDNet</p></td></tr><tr style="height:12pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PSNR</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">18.72</p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">16.04</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">18.81</p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">20.08</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">14.67</p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">30.49</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">31.01</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">32.54</p></td></tr><tr style="height:12pt"><td style="width:38pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">SSIM</p></td><td style="width:39pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.8552</p></td><td style="width:45pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.7825</p></td><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.7971</p></td><td style="width:43pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.8440</p></td><td style="width:57pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.7606</p></td><td style="width:54pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9272</p></td><td style="width:42pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9251</p></td><td style="width:33pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9294</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 4pt;padding-left: 67pt;text-indent: 0pt;text-align: left;"><a name="bookmark55">Table 3</a></h2><p class="s15" style="padding-left: 67pt;text-indent: 0pt;line-height: 106%;text-align: left;">Average PSNR and SSIM values of indoor synthetic dusty image dedusting results obtained by different methods. The data marked in red and blue indicate the best and the second-best quantitative evaluation results, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:67.228pt" cellspacing="0"><tr style="height:14pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Methods</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">LPNet </a><a href="#bookmark74" class="s83">[9]</a></p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark79" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">RGNet </a><a href="#bookmark79" class="s83">[10]</a></p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">AODNet </a><a href="#bookmark75" class="s83">[7]</a></p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFANet </a><a href="#bookmark76" class="s83">[8]</a></p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">Wang et al. </a><a href="#bookmark72" class="s83">[4]</a></p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark82" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">HardGAN </a><a href="#bookmark82" class="s83">[16]</a></p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFNet </a><a href="#bookmark80" class="s83">[12]</a></p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SIDNet</p></td></tr><tr style="height:12pt"><td style="width:38pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">PSNR</p></td><td style="width:39pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">16.37</p></td><td style="width:45pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">19.69</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">17.16</p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">17.17</p></td><td style="width:57pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">15.42</p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">30.83</p></td><td style="width:42pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">27.03</p></td><td style="width:33pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">32.82</p></td></tr><tr style="height:12pt"><td style="width:38pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">SSIM</p></td><td style="width:39pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.8359</p></td><td style="width:45pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.8439</p></td><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.8091</p></td><td style="width:43pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.7894</p></td><td style="width:57pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.7859</p></td><td style="width:54pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9381</p></td><td style="width:42pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9266</p></td><td style="width:33pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">0.9432</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 4pt;padding-left: 65pt;text-indent: 0pt;text-align: left;"><a name="bookmark56">Table 4</a></h2><p class="s15" style="padding-left: 65pt;text-indent: 0pt;line-height: 106%;text-align: left;">Average SSEQ values of real dusty image dedusting results obtained by different methods. The data marked in red and blue indicate the best and the second-best quantitative evaluation results, respectively.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:65.239pt" cellspacing="0"><tr style="height:14pt"><td style="width:42pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">Image No.</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark74" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">LPNet </a><a href="#bookmark74" class="s83">[9]</a></p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark79" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">RGNet </a><a href="#bookmark79" class="s83">[10]</a></p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark75" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">AODNet </a><a href="#bookmark75" class="s83">[7]</a></p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark76" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFANet </a><a href="#bookmark76" class="s83">[8]</a></p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark72" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">Wang et al. </a><a href="#bookmark72" class="s83">[4]</a></p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark82" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">HardGAN </a><a href="#bookmark82" class="s83">[16]</a></p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a href="#bookmark80" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;">FFNet </a><a href="#bookmark80" class="s83">[12]</a></p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SIDNet</p></td></tr><tr style="height:12pt"><td style="width:42pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">1</p></td><td style="width:40pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">55.80</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">53.11</p></td><td style="width:46pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">63.06</p></td><td style="width:44pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">54.21</p></td><td style="width:56pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">53.15</p></td><td style="width:54pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">51.90</p></td><td style="width:43pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">53.10</p></td><td style="width:32pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">58.64</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">2</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.73</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">73.43</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">80.63</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.93</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">75.05</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">74.02</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">85.38</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">87.78</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">3</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">58.56</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.58</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">53.72</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.80</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">71.70</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">56.56</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">53.72</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">74.48</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">4</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">63.63</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">64.84</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">68.07</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">62.24</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">68.13</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">66.61</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">66.70</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">68.49</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">5</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">58.27</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">66.84</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">51.03</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.74</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.39</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">60.81</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">63.80</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">60.76</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">6</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.93</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.96</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.12</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.24</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">74.05</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">66.46</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">66.01</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">75.69</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">7</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">59.92</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">69.27</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.74</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">68.54</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">69.54</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.91</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.30</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">70.69</p></td></tr><tr style="height:9pt"><td style="width:42pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">8</p></td><td style="width:40pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">60.48</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.74</p></td><td style="width:46pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">45.61</p></td><td style="width:44pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">69.53</p></td><td style="width:56pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.84</p></td><td style="width:54pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">55.34</p></td><td style="width:43pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.87</p></td><td style="width:32pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">60.78</p></td></tr><tr style="height:12pt"><td style="width:42pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">Average</p></td><td style="width:40pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">60.79</p></td><td style="width:44pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.34</p></td><td style="width:46pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">61.87</p></td><td style="width:44pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">64.65</p></td><td style="width:56pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">67.35</p></td><td style="width:54pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">62.05</p></td><td style="width:43pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">64.74</p></td><td style="width:32pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">69.66</p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark14">second-highest average SSEQ value. FFNet gains the second-highest SSEQ values on the second and the last images. LPNet exhibits the worst dedusting performance with the lowest average SSEQ value of 60.97.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: justify;"><p class="s21" style="display: inline;">Qualitative evaluation</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-left: 17pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a href="#bookmark57" class="s93">To make a qualitative comparison of different methods, </a><a href="#bookmark57" class="s17">Figs. </a>9<span style=" color: #000;">–</span></p><p class="s18" style="padding-left: 5pt;text-indent: 0pt;text-align: justify;">11 <a href="#bookmark57" class="s93">exhibit visual dedusting results of the eight methods on sev- eral representative synthetic and real dusty images. Speciﬁcally, </a>Figs. 9 <a href="#bookmark58" class="s93">and </a>10 <span style=" color: #000;">exhibit the dedusting performance of different meth-</span></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark59" class="s93">ods on six outdoor and indoor synthetic dusty images, respectively. </a><a href="#bookmark59" class="s17">Fig. </a>11 <span style=" color: #000;">exhibits the dedusting performance of different methods on eight real dusty images.</span></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;text-align: justify;"><a href="#bookmark57" class="s93">From </a><a href="#bookmark57" class="s17">Fig. </a>9<span style=" color: #000;">, it can be observed that the outdoor image de- dusting results obtained by the proposed SIDNet are close to the ground-truth (i.e., dust-free images). HardGAN and FFNet ob- tain better dedusting performances than other reference methods. AODNet fails to remove dust for certain scenes. LPNet, RGNet and FFANet cause different degrees of color distortions. Wang et al. cor- rect dusty image color. However, their dedusting results show low color saturation. For the indoor synthetic image dedusting results,</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 227pt;text-align: left;"><span><img width="608" height="302" alt="image" src="Online版本/Image_021.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 122pt;text-indent: 0pt;text-align: left;"><a name="bookmark57">Fig. 9. </a><span class="s15">Visual image dedusting results of different methods on outdoor synthetic dusty images.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 226pt;text-align: left;"><span><img width="608" height="301" alt="image" src="Online版本/Image_022.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 122pt;text-indent: 0pt;text-align: left;"><a name="bookmark58">Fig. 10. </a><span class="s15">Visual image dedusting results of different methods on indoor synthetic dusty images.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: right;"><a href="#bookmark58" class="s93" name="bookmark15">as shown in </a><a href="#bookmark58" class="s17">Fig. </a>10<a href="#bookmark59" class="s93">, the dedusting results obtained by the SID- Net are more similar to the ground-truth, indicating the best im- age dedusting performances. HardGAN and FFNet remove most of the dust effectively, and their dedusting results are also closed to the ground-truth. The dedusting results obtained by Wang et al. have low color saturation. LPNet, RGNet, and FFANet cause differ- ent color distortions. AODNet removes slight dust and shows low color saturation in some dedusting results, such as the last image. To further validate image dedusting performance, we trained eight methods on synthetic dusty images and tested them on sev- eral real dusty images. </a><a href="#bookmark59" class="s17">Fig. </a>11 <span style=" color: #000;">exhibits dedusting performances of different methods on eight representative real dusty images. These images contain different common dust scenes: forest with heavy dust-storm, multi-detailed trees, desert scene, road with short- distance persons, road with several cars, buildings under the sky, buildings with dense red roofs, and dust-storm outside the win-</span></p><p class="s18" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark59" class="s93">dows. From </a><a href="#bookmark59" class="s17">Fig. </a>11<span style=" color: #000;">, it can ﬁnd that LPNet and AODNet remove slight dust, and dedusting results are darkish. RGNet and FFANet fail to remove most dust and cause serious color distortion. The method of Wang et al. removes most of the dust, however, the color of their dedusting results in low saturation. HardGAN and FFNet achieve better dedusting results than the former ﬁve meth- ods. However, there are still dust residues on their dedusting re- sults, such as the third, the fourth, and the last images. The SIDNet generates a good tradeoff between better image dedusting results and high color saturation.</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 22pt;text-indent: -16pt;text-align: justify;"><p class="s21" style="display: inline;">Potential applications</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;">To validate the image dedusting could beneﬁt other vision tasks, we performed two potential applications, namely image edge extraction and local keypoints matching.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 264pt;text-align: left;"><span><img width="608" height="352" alt="image" src="Online版本/Image_023.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;text-indent: 0pt;text-align: center;"><a name="bookmark59">Fig. 11. </a><span class="s15">Visual image dedusting results of different methods on real dusty images.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 62pt;text-align: left;"><span><img width="608" height="83" alt="image" src="Online版本/Image_024.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-left: 169pt;text-indent: 7pt;text-align: left;"><a name="bookmark60">Fig. 12. </a><span class="s15">Edge extraction results on a real dedusted sample.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 164pt;text-align: left;"><span><img width="608" height="218" alt="image" src="Online版本/Image_025.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="text-indent: 0pt;text-align: center;"><a name="bookmark61">Fig. 13. </a><span class="s15">Number of matching points on a pair of similar scenes.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l7"><li style="padding-top: 3pt;padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s21" style="display: inline;"><a name="bookmark16">Image edge extraction</a><a name="bookmark17">&zwnj;</a></p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark60" class="s93">The color cast and low color contrast of dusty images make the edge extraction of dusty images diﬃcult. To validate the sig- niﬁcance of the SIDNet on image edge extraction performance, we analyzed the performances of image edge extraction on dedusting images of different methods. Compared with other counterparts, the edge details are increased in the dedusting results of the SID- Net as shown in </a><a href="#bookmark60" class="s17">Fig. </a>12<span style=" color: #000;">. This validates that the SIDNet can sig- niﬁcantly improve image edge extraction performance with more details.</span></p></li><li style="padding-top: 3pt;padding-left: 28pt;text-indent: -22pt;text-align: left;"><p class="s21" style="display: inline;">Local keypoints matching</p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="#bookmark96" class="s93">Local keypoints matching aims to ﬁnd correspondences be- tween two similar scenes. To validate the signiﬁcance of the SIDNet on local keypoints matching, we use the SIFT (scale-invariant fea- ture transform) operator </a>[31] <a href="#bookmark61" class="s93">for a pair of similar real heavy dusty images (captured from the same airport), and their correspond- ing dedusting image pairs obtained by different methods. Match- ing results are exhibited in </a><a href="#bookmark61" class="s17">Fig. </a>13<span style=" color: #000;">. One can see that the number of matching keypoints is obviously increased in the dedusting image</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 97pt;text-indent: 0pt;line-height: 119pt;text-align: left;"><span><img width="448" height="159" alt="image" src="Online版本/Image_026.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 49pt;text-indent: 0pt;text-align: left;"><a name="bookmark62">Fig. 14. </a><span class="s15">The brightness comparisons of two real dusty image dedusted samples obtained by MSCNN module and DustAre module, respectively.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 85pt;text-indent: 0pt;line-height: 407pt;text-align: left;"><span><img width="480" height="543" alt="image" src="Online版本/Image_027.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 63pt;text-indent: 0pt;text-align: left;"><a name="bookmark63">Fig. 15. </a><span class="s15">Edge extraction effects of a real dusty image dedusted sample obtained by MSCNN module and DustAre module, respectively.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: left;"><a name="bookmark18">pairs. This conﬁrms that the SIDNet can also recover local features of dusty images.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l8"><ol id="l9"><li style="padding-left: 22pt;text-indent: -16pt;text-align: left;"><p class="s21" style="display: inline;"><a name="bookmark19">Discussions</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><ol id="l10"><li style="padding-left: 27pt;text-indent: -21pt;text-align: left;"><p class="s21" style="display: inline;">Analysis  of  DustAre module</p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: left;">To improve the performances of dedusting results in real dusty image, we introduce the Dust-aware representation extraction</p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">(DustAre) module to the SIDNet, which is mainly used to esti- mation global veiling-light (<span class="s21">V</span>) and local spectral information (<span class="s21">S</span>). For the estimation of <span class="s21">V</span>, since our training dataset is mainly con- centrated on urban scenes (Cityscapes dataset) and indoor scenes (NYU-depth dataset), the illumination information is quite differ- ent from the real dusty scenes. For example, both of Cityscapes and NYU-depth datasets basically do not include the sky area, but more sky areas represent higher brightness. Therefore, in order</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 55pt;text-indent: 0pt;line-height: 188pt;text-align: left;"><span><img width="560" height="251" alt="image" src="Online版本/Image_028.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;padding-left: 158pt;text-indent: 0pt;text-align: left;"><a name="bookmark64">Fig. 16. </a><span class="s15">Impact of color cast correction on the SIDNet dedusting effect.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 37pt;text-indent: 0pt;line-height: 216pt;text-align: left;"><span><img width="608" height="288" alt="image" src="Online版本/Image_029.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 3pt;padding-left: 142pt;text-indent: 0pt;text-align: left;"><a name="bookmark65"><span class="h2">Fig. 17. </span></a>Qualitative comparison results of the SIDNet with different <span class="s85">L</span><span class="s86">ab</span>’s weights.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark20">to make our model more suitable for complex lighting conditions under natural sand-dust storm weather conditions, we introduce the DustAre module to estimate the global veiling-light (</a><span class="s21">V </span>). To prove the superiority of the DustAre module in the estimation of <span class="s21">V</span><a href="#bookmark62" class="s93">, we compared it with other modules (such as Multi-Scale Convolutional Neural Network, MSCNN). The experimental results given in </a><a href="#bookmark62" class="s17">Fig. </a><span style=" color: #0080AC;">14 </span>demonstrate that the MSCNN module is diﬃcult to adapt to scenes with large-area thick dust. On the contrary, the DustAre module has higher brightness on both thin and thick dust scenes.<a name="bookmark66">&zwnj;</a></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;">For the estimation of <span class="s21">S</span>, since a dusty image is different from a general degraded image, it dramatically impacts the scene. More- over, different dust colors have different effects on the seman- tic information of different objects. Therefore, in order to retain the local spatial information of the dedusted results, we intro- duce the DustAre module to estimate <span class="s21">S</span>. To prove the superiority of the DustAre module in the restoration of <span class="s21">S</span><a href="#bookmark63" class="s93">, we compared it with MSCNN. The experimental results given in </a><a href="#bookmark63" class="s17">Fig. </a><span style=" color: #0080AC;">15 </span>demonstrate that the edge extraction result of the DustAre module can restore more image details than the MSCNN module.</p><h2 style="padding-top: 4pt;padding-left: 53pt;text-indent: 0pt;text-align: justify;">Table 5</h2><p class="s15" style="padding-left: 53pt;text-indent: 0pt;line-height: 106%;text-align: justify;">Impact of color cast correction on training label im- ages of the SIDNet for image dedusting. The data marked in red indicate the best quantitative eval- uation results.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:53.044pt" cellspacing="0"><tr style="height:14pt"><td style="width:103pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">The SIDNet</p></td><td style="width:28pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">SSEQ</p></td><td style="width:25pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">E</p></td></tr><tr style="height:12pt"><td style="width:103pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">Without color cast correction</p></td><td style="width:28pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">68.72</p></td><td style="width:25pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 6pt;text-indent: 0pt;text-align: left;">7.37</p></td></tr><tr style="height:11pt"><td style="width:103pt"><p class="s87" style="text-indent: 0pt;line-height: 8pt;text-align: left;">    With color cast correction               </p></td><td style="width:28pt"><p class="s87" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;">69.70        </p></td><td style="width:25pt"><p class="s87" style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;">7.45    </p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 28pt;text-indent: -22pt;text-align: left;"><p class="s21" style="display: inline;">Analysis of color cast correction  strategy</p><p class="s18" style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><span style=" color: #000;">To better remove dust from real dusty images and maintain more natural scene colors, the proposed SIDNet corrected the color of the ground-truth images of the training dataset by giving a cer- tain threshold </span><span class="s21">K</span><span class="s39">=</span><a href="#bookmark66" class="s93">1.5. To validate the impact of color cast correc- tion for ground-truth on real images dedusting, we tested on 20 real dusty images and exhibited two representative dedusting re- sults obtained by the SIDNet without and with color cast correc- tion, respectively. The quantitative and qualitative comparison re- sults are given in </a><a href="#bookmark66" class="s17">Table </a>5 <a href="#bookmark64" class="s93">and </a><a href="#bookmark64" class="s17">Fig. </a>16<span style=" color: #000;">. Experimental results show</span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 97pt;text-indent: 0pt;line-height: 541pt;text-align: left;"><span><img width="448" height="721" alt="image" src="Online版本/Image_030.jpg"/></span></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h2 style="padding-top: 3pt;text-indent: 0pt;text-align: center;"><a name="bookmark67">Fig. 18. </a><span class="s15">Training error curve.</span></h2><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a name="bookmark21">that the SIDNet with color cast correction for ground-truth images generates higher average SSEQ and E values and improves the im- age dedusting effect with more natural scene colors.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 28pt;text-indent: -22pt;line-height: 10pt;text-align: justify;"><p class="s21" style="display: inline;">Analysis  of  <span class="s68">L</span><span class="s44">ab</span>’s weight</p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 91%;text-align: justify;"><a name="bookmark22">Motivated by the observation that a dusty image usually has larger </a><span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>average chromaticity values in CIELAB color space than that of a dust-free image, the SIDNet designs a new loss item <span class="s68">L</span><span class="s44">ab</span><span class="s13"> </span><a href="#bookmark50" class="s93">in </a><a href="#bookmark50" class="s17">Eq. </a><span style=" color: #0080AC;">(18)</span>. To validate the impact of <span class="s68">L</span><span class="s44">ab</span><span class="s13"> </span><a href="#bookmark97" class="s93">on the dedusting re- sults, we followed the quantitative setting of 10 times difference in </a><span style=" color: #0080AC;">[32]</span>, and trained SIDNet with <span class="s25">λ</span><span class="s44">ab</span><span class="s13"> </span>as 0, 3 <span class="s39">× </span>10<span class="s22">−</span><span class="s15">3</span>, 3 <span class="s39">× </span>10<span class="s22">−</span><span class="s15">2</span>, 3 <span class="s39">× </span>10<span class="s22">−</span><span class="s15">1</span></p><p style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark69" class="s93">and 3, respectively. Then, all models were evaluated on 20 real dusty images. The quantitative and qualitative comparison results are given in </a><a href="#bookmark69" class="s17">Table </a><span style=" color: #0080AC;">6 </span><a href="#bookmark65" class="s93">and </a><a href="#bookmark65" class="s17">Fig. </a><span style=" color: #0080AC;">17</span>. As shown, when <span class="s25">λ</span><span class="s44">ab</span><span class="s13"> </span><span class="s39">= </span>0<span class="s45">.</span>3, the SIDNet generates higher average SSEQ and E values, improves the local dark shadow, and maintains the original high bright region without overexposure. However, others are getting too dark local shadow.</p><p style="text-indent: 0pt;text-align: left;"><br/></p></li><li style="padding-left: 28pt;text-indent: -22pt;line-height: 9pt;text-align: justify;"><p class="s21" style="display: inline;">Analysis of training error  curve</p></li></ol></li></ol></ol><p style="padding-left: 17pt;text-indent: 0pt;line-height: 12pt;text-align: left;">After analysis of <span class="s68">L</span><span class="s44">ab</span>’s weight, we apply a more acceptable <span class="s25">λ</span><span class="s44">ab</span></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: justify;">value of 0.3 for our model training. The training loss curve is</p><h2 style="padding-top: 3pt;padding-left: 47pt;text-indent: 0pt;text-align: justify;"><a name="bookmark27">Table 6</a><a name="bookmark68">&zwnj;</a><a name="bookmark69">&zwnj;</a></h2><p class="s15" style="padding-left: 47pt;text-indent: 0pt;text-align: justify;"><a name="bookmark70">Quantitative comparison results of the SIDNet with dif- ferent </a><span class="s85">L</span><span class="s86">ab</span>’s weights. The data marked in red indicate the best quantitative evaluation results.</p><p style="text-indent: 0pt;text-align: left;"><br/></p><table style="border-collapse:collapse;margin-left:47.446pt" cellspacing="0"><tr style="height:14pt"><td style="width:62pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 5pt;text-indent: 0pt;text-align: left;"><a name="bookmark71">the SIDNet</a></p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;text-indent: 0pt;text-align: center;">SSEQ</p></td><td style="width:41pt;border-top-style:solid;border-top-width:1pt;border-bottom-style:solid;border-bottom-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;text-align: center;">E</p></td></tr><tr style="height:12pt"><td style="width:62pt;border-top-style:solid;border-top-width:1pt"><p class="s88" style="padding-top: 1pt;padding-left: 5pt;text-indent: 0pt;line-height: 10pt;text-align: left;"><a name="bookmark72">λ</a><span class="s89">ab </span><span class="s84">= </span><span class="s82">0</span></p></td><td style="width:64pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 1pt;text-indent: 0pt;text-align: center;">67.16</p></td><td style="width:41pt;border-top-style:solid;border-top-width:1pt"><p class="s82" style="padding-top: 2pt;padding-left: 22pt;text-indent: 0pt;text-align: left;">6.97</p></td></tr><tr style="height:9pt"><td style="width:62pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">λ<span class="s89">ab </span><span class="s84">= </span><span class="s82">0.003</span></p></td><td style="width:64pt"><p class="s82" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">68.34</p></td><td style="width:41pt"><p class="s82" style="padding-left: 22pt;text-indent: 0pt;line-height: 8pt;text-align: left;">7.03</p></td></tr><tr style="height:9pt"><td style="width:62pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">λ<span class="s89">ab </span><span class="s84">= </span><span class="s82">0.03</span></p></td><td style="width:64pt"><p class="s82" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">68.33</p></td><td style="width:41pt"><p class="s82" style="padding-left: 22pt;text-indent: 0pt;line-height: 8pt;text-align: left;">7.06</p></td></tr><tr style="height:9pt"><td style="width:62pt"><p class="s88" style="padding-left: 5pt;text-indent: 0pt;line-height: 9pt;text-align: left;">λ<span class="s89">ab </span><span class="s84">= </span><span class="s82">0.3</span></p></td><td style="width:64pt"><p class="s82" style="padding-left: 1pt;text-indent: 0pt;line-height: 8pt;text-align: center;">69.70</p></td><td style="width:41pt"><p class="s82" style="padding-left: 22pt;text-indent: 0pt;line-height: 8pt;text-align: left;">7.45</p></td></tr><tr style="height:11pt"><td style="width:62pt"><p class="s90" style="text-indent: 0pt;line-height: 9pt;text-align: left;"><a name="bookmark73"> </a><span class="s88"> </span>λ<span class="s91">ab </span><span class="s92">= </span><span class="s87">3                                       </span></p></td><td style="width:64pt"><p class="s87" style="padding-left: 24pt;text-indent: 0pt;line-height: 8pt;text-align: left;">65.34                              </p></td><td style="width:41pt"><p class="s87" style="padding-left: 22pt;text-indent: 0pt;line-height: 8pt;text-align: left;">6.69    </p></td></tr></table><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s18" style="padding-top: 4pt;padding-left: 5pt;text-indent: 0pt;line-height: 11pt;text-align: justify;"><a href="#bookmark67" class="s93" name="bookmark23">shown in </a><a href="#bookmark67" class="s17">Fig. </a>18<a href="#bookmark67" class="s93">(a), and it can be observed that the SIDNet con- verges quickly and easily. Additionally, to demonstrate the eﬃ- ciency of our model, we further give the PSNR curve during the training in </a><a href="#bookmark67" class="s17">Fig. </a>18<span style=" color: #000;">(b). All experiments are performed on three NVIDIA Tesla P100 GPUs.</span><a name="bookmark74">&zwnj;</a><a name="bookmark75">&zwnj;</a><a name="bookmark76">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p></li></ol></li></ol></li><li style="padding-left: 16pt;text-indent: -10pt;text-align: justify;"><h1 style="display: inline;"><a name="bookmark77">Conclusion</a><a name="bookmark78">&zwnj;</a><a name="bookmark79">&zwnj;</a></h1></li></ol><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a name="bookmark24">To remove the dust from a dusty image and make the scene clear, this paper proposes an effective dedusting network for sin- gle image (SIDNet). The SIDNet develops several dust-aware rep- resentation extraction (DustAre) modules with the same structure. Each DustAre module contains two branches. The ﬁrst branch esti- mates global veiling-light and local spatial information. The second branch generates a dust-aware map, and fuses the global veiling- light and the local spatial information to generate the output. Moreover, the SIDNet introduces the color cast correction scheme to the neural network for further improving the image dedust- ing performance on real dusty images. Under the consideration of the differences of </a><span class="s21">a</span><span class="s22">∗</span><span class="s23"> </span>and <span class="s21">b</span><span class="s22">∗</span><span class="s23"> </span>components between a dusty image and a dust-free image in CIELAB color space, the SIDNet designs a new loss function to better guide network training. In addition, af- ter considering the relationship of the scene depth between a real dusty image and a dust-free image, we developed a novel dusty image synthesis method for enriching dusty image datasets. Ex- perimental results on synthetic dusty images and real dusty im- ages show that the SIDNet dramatically improves dedusting perfor- mance than the-state-of-art methods. In the future, we will further explore semi-supervised deep learning networks for dusty image synthesis and image dedusting.<a name="bookmark80">&zwnj;</a><a name="bookmark81">&zwnj;</a><a name="bookmark82">&zwnj;</a><a name="bookmark83">&zwnj;</a><a name="bookmark84">&zwnj;</a><a name="bookmark85">&zwnj;</a><a name="bookmark86">&zwnj;</a><a name="bookmark87">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark88">Declaration of Competing Interest</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a name="bookmark25">The authors declare that they have no known competing ﬁnan- cial interests or personal relationships that could have appeared to inﬂuence the work reported in this paper.</a><a name="bookmark89">&zwnj;</a><a name="bookmark90">&zwnj;</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark91">CRediT authorship contribution statement</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a name="bookmark26">Jiayan Huang: </a><span class="p">Conceptualization, Methodology, Software, Writ- ing – original draft, Writing – review &amp; editing. </span>Haiping Xu: <span class="p">Data curation, Visualization, Supervision. </span>Guanghai Liu: <span class="p">Validation, Su- pervision. </span>Chuansheng Wang: <span class="p">Formal analysis, Investigation, Data curation, Validation, Supervision. </span>Zhongyi Hu: <span class="p">Visualization, Su- pervision, Funding acquisition. </span>Zuoyong Li: <span class="p">Resources, Supervision, Funding acquisition.</span><a name="bookmark92">&zwnj;</a><a name="bookmark93">&zwnj;</a><a name="bookmark94">&zwnj;</a><a name="bookmark95">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><h1 style="padding-left: 5pt;text-indent: 0pt;text-align: justify;"><a name="bookmark96">Acknowledgments</a><a name="bookmark97">&zwnj;</a></h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 5pt;text-indent: 11pt;line-height: 11pt;text-align: justify;"><a href="https://doi.org/10.13039/501100001809" class="s93" target="_blank" name="bookmark98">This work is partially supported by </a><a href="https://doi.org/10.13039/501100003392" class="s17" target="_blank">National Natural Science Foundation of China </a><a href="#bookmark98" class="s93">(</a><a href="https://doi.org/10.13039/501100003392" class="s17" target="_blank">61972187</a><a href="#bookmark98" class="s93">, </a><a href="https://doi.org/10.13039/501100003392" class="s17" target="_blank">61866005</a><a href="https://doi.org/10.13039/501100003392" class="s93" target="_blank">), </a><a href="https://doi.org/10.13039/501100013092" class="s17" target="_blank">Natural Science Foun- dation of Fujian Province </a><a href="#bookmark99" class="s93">(</a><a href="https://doi.org/10.13039/501100013092" class="s17" target="_blank">2020J02024 </a><a href="#bookmark99" class="s93">and </a><a href="https://doi.org/10.13039/501100013092" class="s17" target="_blank">2019J01756</a><a href="https://doi.org/10.13039/501100013092" class="s93" target="_blank">), </a><a href="https://doi.org/10.13039/501100013092" class="s17" target="_blank">Fuzhou Science and Technology Project </a><a href="#bookmark100" class="s93">(</a><a href="https://doi.org/10.13039/501100013092" class="s17" target="_blank">2020-RC-186</a><a href="https://doi.org/10.13039/501100013092" class="s93" target="_blank">), and Key </a>Project of Zhejiang Provincial Natural Science Foundation (LD21F020001).<a name="bookmark99">&zwnj;</a><a name="bookmark100">&zwnj;</a></p><h1 style="padding-top: 3pt;padding-left: 5pt;text-indent: 0pt;text-align: left;">References</h1><p style="text-indent: 0pt;text-align: left;"><br/></p><p style="padding-left: 21pt;text-indent: -11pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0001" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[1] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0001" class="a" target="_blank">G. Liu, J. Yang, Deep-seated features histogram: a novel image retrieval method, Pattern Recognit. 116 (2021) 107926.</a></p><p style="padding-left: 21pt;text-indent: -11pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0002" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[2] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0002" class="a" target="_blank">Y. Zheng, J. Fan, J. Zhang, X.-B. Gao, Exploiting related and unrelated tasks for hierarchical metric learning and image classiﬁcation, IEEE Trans. Image Pro- cess. 29 (1) (2020) 883–896.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0003" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[3] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0003" class="a" target="_blank">G. Liu, J. Yang, Exploiting color volume and color difference for salient region detection, IEEE Trans. Image Process. 28 (1) (2019) 6–16.</a></p><p style="padding-left: 9pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0004" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[4] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0004" class="a" target="_blank">J. Wang, Y. Pang, Y. He, C. Liu, Enhancement for dust-sand storm images, Int.</a></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0004" class="a" target="_blank">Conf. Multimedia Model. (2016) 842–849.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0005" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[5] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0005" class="a" target="_blank">G. Gao, H. Lai, Z. Jia, Y. Liu, Y. Wang, Sand-dust image restoration based on reversing the blue channel prior, IEEE Photonics J. 12 (2) (2020) 1–16.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0006" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[6] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0006" class="a" target="_blank">C. Wang, Z. Li, J. Wu, H. Fan, G. Xiao, H. Zhang, Deep residual haze network for image dehazing and deraining, IEEE Access 8 (2020) 9488–9500.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0007" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[7] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0007" class="a" target="_blank">B. Li, X. Peng, Z. Wang, J. Xu, D. Feng, AOD-Net: all-in-one dehazing network, IEEE Int. Conf. Comput. Vision (2017) 4770–4778.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0008" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[8] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0008" class="a" target="_blank">X. Qin, Z. Wang, Y. Bai, X. Xie, H. Jia, FFA-Net: feature fusion attention net- work for single image dehazing, AAAI Conf. Artif. Intell. 34 (7) (2020) 11908–</a></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0008" class="a" target="_blank">11915.</a></p><p style="padding-left: 21pt;text-indent: -11pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0009" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[9] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0009" class="a" target="_blank">X. Fu, B. Liang, Y. Huang, X. Ding, J. Paisley, Lightweight pyramid networks for image deraining, IEEE Trans. Neural Netw. Learn. Syst. 31 (6) (2019) 1794–</a></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0009" class="a" target="_blank">1807.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0010" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[10] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0010" class="a" target="_blank">Z. Fan, H. Wu, X. Fu, Y. Huang, X. Ding, Residual-guide network for single im- age deraining, 26th ACM Int. Conf. Multimedia (2018) 1751–1759.</a></p><p style="padding-left: 21pt;text-indent: -14pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0011" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[11] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0011" class="a" target="_blank">D. Berman, D. Levy, S. Avidan, T. Treibitz, Underwater single image color restoration using haze-lines and a new quantitative dataset, IEEE Trans. Pat- tern Anal. Mach. Intell. 43 (8) (2021) 2822–2837.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0012" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[12] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0012" class="a" target="_blank">J. Huang, Z. Li, C. Wang, Z. Yu, X. Cao, FFNet: a simple image dedusting network with feature fusion, Concurrency Comput. (2021) e6462.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0013" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[13] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0013" class="a" target="_blank">K.M. Jeong, B.C. Song, Fog detection and fog synthesis for effective quantitative evaluation of fog–detection-and-removal algorithms, IEIE Trans. Smart Process. Comput. 7 (5) (2018) 350–360.</a></p><p style="padding-left: 6pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0014" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[14] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0014" class="a" target="_blank">F. Gasparini, R. Schettini, Color correction for digital photographs, 12th Int.</a></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0014" class="a" target="_blank">Conf. Image Anal. Process. (2003) 646–651.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0015" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[15] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0015" class="a" target="_blank">K. Iqbal, M. Odetayo, A. James, R.A. Salam, A.Z.H. Talib, Enhancing the low qual- ity images using unsupervised colour correction method, IEEE Int. Conf. Syst. Man Cybern. (2010) 1703–1709.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0016" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[16] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0016" class="a" target="_blank">Q. Deng, Z. Huang, C.-C. Tsai, C.-W. Lin, HardGAN: a haze-aware representation distillation GAN for single image dehazing, Eur. Conf. Comput. Vision (2020) 722–738.</a></p><p style="padding-left: 21pt;text-indent: -14pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0017" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[17] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0017" class="a" target="_blank">H. Koschmieder, Theorie der horizontalen sichtweite, beitrage zur physik der freien atmosphare, Meteorol. Z. 12 (1924) 3353.</a></p><p class="s20" style="padding-left: 6pt;text-indent: -3pt;text-align: center;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0018" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[18] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0018" class="a" target="_blank">R. Fattal, Single image dehazing, ACM Trans. Graph. (TOG) 27 (3) (2008) </a>1–9. <a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0019" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[19] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0019" class="a" target="_blank">C. Godard, O. Mac Aodha, M. Firman, G.J. Brostow, Digging into self-super-</a></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0019" class="a" target="_blank">vised monocular depth estimation, IEEE/CVF Int. Conf. Comput. Vision (2019) 3828–3838.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0020" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[20] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0020" class="a" target="_blank">W. Wang, X. Yue, H. Liu, Z. Pan, D. Tang, Y. Wang, D.U. Raoguo, S.U. Hong- mei, F. Qian, S. Kazuhiko, Study on pollution characteristics of aerosols dur- ing sand-dust storm weather in beijing, Acta Scientiae Circumstantiae 22 (4) (2002) 494–498.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0021" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[21] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0021" class="a" target="_blank">B. Cai, X. Xu, K. Jia, C. Qing, D. Tao, DehazeNet: an end-to-end system for single image haze removal, IEEE Trans. Image Process. 25 (11) (2016) 5187–5198.</a></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0022" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[22] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0022" class="a" target="_blank">M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,</a></p><p style="padding-left: 21pt;text-indent: 0pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0022" class="a" target="_blank">U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic urban scene understanding, IEEE Conf. Comput. Vision Pattern Recognit. (2016) 3213– 3223.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0023" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[23] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0023" class="a" target="_blank">N. Silberman, D. Hoiem, P. Kohli, R. Fergus, Indoor segmentation and sup- port inference from RGBD images, Eur. Conf. Comput. Vision (2012) 746– 760.</a></p><p class="s20" style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0024" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[24] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0024" class="a" target="_blank">R. Girshick, Fast R-CNN, IEEE Int. Conf. Comput. Vision (2015) </a>1440–1448.</p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0025" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[25] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0025" class="a" target="_blank">O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpa- thy, A. Khosla, M. Bernstein, et al., ImageNet large scale visual recognition chal- lenge, Int. J. Comput. Vis. 115 (3) (2015) 211–252.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0026" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[26] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0026" class="a" target="_blank">H. Zhang, V.M. Patel, Density-aware single image de-raining using a mul- ti-stream dense network, IEEE/CVF Conf. Comput. Vision Pattern Recognit. (2018) 695–704.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0027" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[27] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0027" class="a" target="_blank">R. Li, S. You, X. Zhang, Y. Li, Learning to dehaze from realistic scene with a fast physics-based dehazing network, arXiv preprint arXiv:2004.08554 (2020).</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0028" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[28] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0028" class="a" target="_blank">Z. Wang, A.C. Bovik, H.R. Sheikh, E.P. Simoncelli, Image quality assessment: from error visibility to structural similarity, IEEE Trans. Image Process. 13 (4) (2004) 600–612.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0029" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[29] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0029" class="a" target="_blank">L. Liu, B. Liu, H. Huang, A.C. Bovik, No-reference image quality assessment based on spatial and spectral entropies, Signal Process. Image Commun. 29 (8) (2014) 856–863.</a></p><p style="padding-left: 5pt;text-indent: 0pt;line-height: 8pt;text-align: left;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0030" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[30] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0030" class="a" target="_blank">C.E. Shannon, A mathematical theory of communication, ACM SIGMOBILE Mob.</a></p><p style="padding-left: 21pt;text-indent: 0pt;line-height: 8pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0030" class="a" target="_blank">Comput. Commun. Rev. 5 (1) (2001) 3–55.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0031" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[31] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0031" class="a" target="_blank">D.G. Lowe, Distinctive image features from scale-invariant keypoints, Int. J. Comput. Vis. 60 (2) (2004) 91–110.</a></p><p style="padding-left: 21pt;text-indent: -15pt;text-align: justify;"><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0032" style=" color: black; font-family:&quot;Palatino Linotype&quot;, serif; font-style: normal; font-weight: normal; text-decoration: none; font-size: 6pt;" target="_blank">[32] </a><a href="http://refhub.elsevier.com/S0165-1684(22)00152-9/sbref0032" class="a" target="_blank">S. Zhao, L. Zhang, Y. Shen, Y. Zhou, ReﬁneDNet: a weakly supervised reﬁnement framework for single image dehazing, IEEE Trans. Image Process. 30 (2021) 3391–4340.</a></p><p style="text-indent: 0pt;text-align: left;"><br/></p><p class="s15" style="padding-top: 3pt;padding-left: 120pt;text-indent: 0pt;text-align: center;">14</p></body></html>
